{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport gym\n\nclass ValueNetwork(nn.Module):\n    def __init__(self, num_features, hidden_size, learning_rate=0.01):\n        '''\n        Tác dụng: Khởi tạo mạng giá trị để ước lượng giá trị trạng thái.\n        Tham số:\n            num_features: số chiều đầu vào (đặc trưng trạng thái)\n            hidden_size: kích thước lớp ẩn\n            learning_rate: tốc độ học cho bộ tối ưu\n        Trả về: None\n        '''\n        super(ValueNetwork, self).__init__()\n        self.network = nn.Sequential(\n            nn.Linear(num_features, hidden_size),\n            nn.Sigmoid(),\n            nn.Linear(hidden_size, 1)\n        )\n        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.to(self.device)\n\n    def forward(self, states):\n        '''\n        Tác dụng: Dự đoán giá trị trạng thái.\n        Tham số:\n            states: trạng thái đầu vào (tensor hoặc ndarray)\n        Trả về: Giá trị trạng thái (tensor)\n        '''\n        if not isinstance(states, torch.Tensor):\n            states = torch.as_tensor(states, dtype=torch.float32).to(self.device)\n        return self.network(states).squeeze(-1)\n\n    def update(self, states, discounted_rewards):\n        '''\n        Tác dụng: Cập nhật mạng giá trị bằng MSE Loss giữa giá trị dự đoán và phần thưởng chiết khấu.\n        Tham số:\n            states: tập trạng thái\n            discounted_rewards: phần thưởng chiết khấu tương ứng\n        Trả về: Giá trị loss\n        '''\n        if not isinstance(states, torch.Tensor):\n            states = torch.as_tensor(states, dtype=torch.float32).to(self.device)\n        if not isinstance(discounted_rewards, torch.Tensor):\n            discounted_rewards = torch.as_tensor(discounted_rewards, dtype=torch.float32).to(self.device)\n        self.optimizer.zero_grad()\n        predictions = self(states)\n        loss = nn.MSELoss()(predictions, discounted_rewards)\n        loss.backward()\n        self.optimizer.step()\n        return loss.item()\n\nclass PPOPolicyNetwork(nn.Module):\n    def __init__(self, num_features, layer_1_size, layer_2_size, layer_3_size, num_actions, epsilon=0.2, learning_rate=9e-4):\n        '''\n        Tác dụng: Khởi tạo mạng chính sách PPO với kiến trúc 3 lớp ẩn và softmax đầu ra.\n        Tham số:\n            num_features: số chiều đầu vào (trạng thái)\n            layer_1_size, layer_2_size, layer_3_size: kích thước các lớp ẩn\n            num_actions: số lượng hành động\n            epsilon: ngưỡng cắt trong PPO\n            learning_rate: tốc độ học\n        Trả về: None\n        '''\n        super(PPOPolicyNetwork, self).__init__()\n        self.epsilon = epsilon\n        self.network = nn.Sequential(\n            nn.Linear(num_features, layer_1_size),\n            nn.ReLU(),\n            nn.Linear(layer_1_size, layer_2_size),\n            nn.ReLU(),\n            nn.Linear(layer_2_size, layer_3_size),\n            nn.ReLU(),\n            nn.Linear(layer_3_size, num_actions),\n            nn.Softmax(dim=-1)\n        )\n        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.to(self.device)\n\n    def forward(self, states):\n        '''\n        Tác dụng: Trả về phân phối xác suất hành động.\n        Tham số:\n            states: trạng thái đầu vào\n        Trả về: phân phối xác suất hành động (tensor)\n        '''\n        if not isinstance(states, torch.Tensor):\n            states = torch.as_tensor(states, dtype=torch.float32).to(self.device)\n        return self.network(states)\n\n    def update(self, states, chosen_actions, ep_advantages, old_probabilities):\n        '''\n        Tác dụng: Cập nhật mạng chính sách theo thuật toán PPO.\n        Tham số:\n            states: tập trạng thái\n            chosen_actions: hành động đã chọn dưới dạng one-hot\n            ep_advantages: lợi thế thu được\n            old_probabilities: phân phối xác suất hành động cũ\n        Trả về: giá trị loss\n        '''\n        if not isinstance(states, torch.Tensor):\n            states = torch.as_tensor(states, dtype=torch.float32).to(self.device)\n        if not isinstance(chosen_actions, torch.Tensor):\n            chosen_actions = torch.as_tensor(chosen_actions, dtype=torch.float32).to(self.device)\n        if not isinstance(ep_advantages, torch.Tensor):\n            ep_advantages = torch.as_tensor(ep_advantages, dtype=torch.float32).to(self.device)\n        if not isinstance(old_probabilities, torch.Tensor):\n            old_probabilities = torch.as_tensor(old_probabilities, dtype=torch.float32).to(self.device)\n\n        self.optimizer.zero_grad()\n        new_probabilities = self(states)\n        new_responsible_outputs = (chosen_actions * new_probabilities).sum(dim=1)\n        old_responsible_outputs = (chosen_actions * old_probabilities).sum(dim=1)\n        ratio = new_responsible_outputs / (old_responsible_outputs + 1e-10)\n        clipped_ratio = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon)\n        loss = -torch.min(ratio * ep_advantages, clipped_ratio * ep_advantages).mean()\n        loss.backward()\n        self.optimizer.step()\n        return loss.item()\n\nclass PPO:\n    def __init__(self, env, num_features=1, num_actions=1, gamma=0.98, lam=1, epsilon=0.2,\n                 value_network_lr=0.001, policy_network_lr=0.01, value_network_hidden_size=100,\n                 policy_network_hidden_size_1=40, policy_network_hidden_size_2=35, policy_network_hidden_size_3=30):\n        ''''\n        Tác dụng: Khởi tạo agent PPO với mạng giá trị và mạng chính sách tương ứng.\n        Tham số:\n            env: môi trường huấn luyện (Gym environment)\n            num_features: số lượng đặc trưng (input size của mạng)\n            num_actions: số hành động có thể chọn trong môi trường\n            gamma: hệ số chiết khấu phần thưởng tương lai\n            lam: hệ số lambda trong tính toán lợi thế GAE\n            epsilon: ngưỡng cắt tỉ lệ trong PPO để tránh cập nhật quá mạnh\n            value_network_lr: tốc độ học của mạng giá trị\n            policy_network_lr: tốc độ học của mạng chính sách\n            value_network_hidden_size: kích thước lớp ẩn trong mạng giá trị\n            policy_network_hidden_size_1: kích thước lớp ẩn 1 trong mạng chính sách\n            policy_network_hidden_size_2: kích thước lớp ẩn 2 trong mạng chính sách\n            policy_network_hidden_size_3: kích thước lớp ẩn 3 trong mạng chính sách\n        Trả về: None\n        '''\n        self.env = env\n        self.num_features = num_features\n        self.num_actions = num_actions\n        self.gamma = gamma\n        self.lam = lam\n        self.Pi = PPOPolicyNetwork(\n            num_features=num_features, num_actions=num_actions,\n            layer_1_size=policy_network_hidden_size_1,\n            layer_2_size=policy_network_hidden_size_2,\n            layer_3_size=policy_network_hidden_size_3,\n            epsilon=epsilon, learning_rate=policy_network_lr\n        )\n        self.V = ValueNetwork(num_features, value_network_hidden_size, learning_rate=value_network_lr)\n\n    def discount_rewards(self, rewards):\n        '''\n        Tác dụng: Tính phần thưởng chiết khấu từ danh sách phần thưởng\n        Tham số:\n            rewards: danh sách phần thưởng\n        Trả về: phần thưởng chiết khấu cùng chiều\n        '''\n        running_total = 0\n        discounted = np.zeros_like(rewards)\n        for r in reversed(range(len(rewards))):\n            running_total = running_total * self.gamma + rewards[r]\n            discounted[r] = running_total\n        return discounted\n\n    def calculate_advantages(self, rewards, values):\n        '''\n        Tác dụng: Tính lợi thế bằng công thức GAE (Generalized Advantage Estimation)\n        Tham số:\n            rewards: danh sách phần thưởng\n            values: giá trị trạng thái tương ứng\n        Trả về: lợi thế chuẩn hóa (advantage)\n        '''\n        advantages = np.zeros_like(rewards, dtype=np.float32)\n        for t in range(len(rewards)):\n            ad = 0\n            for l in range(0, len(rewards) - t):\n                delta = rewards[t + l] + (self.gamma * values[t + l + 1] if t + l + 1 < len(values) else 0) - values[t + l]\n                ad += ((self.gamma * self.lam) ** l) * delta\n            advantages[t] = ad\n        return (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-10)\n\n    def run_model(self):\n        '''\n        Tác dụng: Huấn luyện mô hình PPO trên môi trường đã khởi tạo.\n        '''\n        episode = 1\n        running_reward = []\n        environment_solved = False\n        while not environment_solved:\n            reset_result = self.env.reset()\n            s0 = reset_result[0] if isinstance(reset_result, tuple) else reset_result\n            is_terminal = False\n            ep_rewards = []\n            ep_actions = []\n            ep_states = []\n            score = 0\n            while not is_terminal:\n                dist = self.Pi(np.array(s0)[None, :]).detach().cpu().numpy()[0]\n                action = np.random.choice(range(self.num_actions), p=dist)\n                a_binarized = np.zeros(self.num_actions)\n                a_binarized[action] = 1\n                s1, r, terminated, truncated, _ = self.env.step(action)\n                is_terminal = terminated or truncated\n                score += r\n                ep_actions.append(a_binarized)\n                ep_rewards.append(r)\n                ep_states.append(s0)\n                s0 = s1\n                if is_terminal:\n                    ep_actions = np.array(ep_actions, dtype=np.float32)\n                    ep_rewards = np.array(ep_rewards, dtype=np.float32)\n                    ep_states = np.array(ep_states, dtype=np.float32)\n                    targets = self.discount_rewards(ep_rewards)\n                    for i in range(len(ep_states)):\n                        self.V.update(ep_states[i:i+1], targets[i:i+1])\n                    ep_values = self.V(ep_states).detach().cpu().numpy()\n                    ep_advantages = self.calculate_advantages(ep_rewards, ep_values)\n                    old_probabilities = self.Pi(ep_states).detach().cpu().numpy()\n                    self.Pi.update(ep_states, ep_actions, ep_advantages, old_probabilities)\n                    running_reward.append(score)\n                    if episode % 25 == 0:\n                        avg_score = np.mean(running_reward[-25:])\n                        print(f\"Episode: {episode} - Score: {avg_score}\")\n                        if avg_score >= 500:\n                            if not environment_solved:\n                                print(\"Environment solved!\")\n                                environment_solved = True\n                            break\n                    episode += 1","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T10:17:27.913719Z","iopub.execute_input":"2025-05-01T10:17:27.914229Z","iopub.status.idle":"2025-05-01T10:17:27.937619Z","shell.execute_reply.started":"2025-05-01T10:17:27.914204Z","shell.execute_reply":"2025-05-01T10:17:27.937082Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Khởi tạo môi trường và chạy mô hình\nenv = gym.make('CartPole-v1', new_step_api=True)\nagent = PPO(\n    env, num_features=4, num_actions=2, gamma=0.98, lam=1, epsilon=0.2,\n    value_network_lr=0.001, policy_network_lr=0.01, value_network_hidden_size=100,\n    policy_network_hidden_size_1=40, policy_network_hidden_size_2=35, policy_network_hidden_size_3=30\n)\nagent.run_model()\nenv.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T10:17:30.540727Z","iopub.execute_input":"2025-05-01T10:17:30.541009Z","iopub.status.idle":"2025-05-01T10:20:48.314877Z","shell.execute_reply.started":"2025-05-01T10:17:30.540989Z","shell.execute_reply":"2025-05-01T10:20:48.314082Z"}},"outputs":[{"name":"stdout","text":"Episode: 25 - Score: 16.2\nEpisode: 50 - Score: 13.2\nEpisode: 75 - Score: 23.92\nEpisode: 100 - Score: 76.12\nEpisode: 125 - Score: 171.96\nEpisode: 150 - Score: 167.72\nEpisode: 175 - Score: 393.64\nEpisode: 200 - Score: 478.88\nEpisode: 225 - Score: 497.96\nEpisode: 250 - Score: 434.88\nEpisode: 275 - Score: 446.4\nEpisode: 300 - Score: 419.52\nEpisode: 325 - Score: 500.0\nEnvironment solved!\n","output_type":"stream"}],"execution_count":14}]}