{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11849833,"sourceType":"datasetVersion","datasetId":7445719}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.system(\"pip install sentencepiece evaluate sacrebleu rouge_score\")\nprint(\"Dependencies installed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:57:15.698371Z","iopub.execute_input":"2025-05-18T08:57:15.698828Z","iopub.status.idle":"2025-05-18T08:57:19.504781Z","shell.execute_reply.started":"2025-05-18T08:57:15.698794Z","shell.execute_reply":"2025-05-18T08:57:19.504074Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\nRequirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\nCollecting sacrebleu\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51.8/51.8 kB 1.1 MB/s eta 0:00:00\nRequirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 104.1/104.1 kB 2.9 MB/s eta 0:00:00\nDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-3.1.1 sacrebleu-2.5.1\nDependencies installed.\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"import unicodedata\nimport re\nimport html\nfrom sklearn.model_selection import train_test_split\nimport pickle\n\ndef preprocess(text):\n    text = unicodedata.normalize(\"NFC\", html.unescape(text))\n    text = text.strip()\n    text = re.sub(r\"\\s+\", \" \", text)\n    return text\n\ndef prepare_and_split_data(en_file, vi_file, output_dir=\"data\", train_ratio=0.8, val_ratio=0.1):\n    os.makedirs(output_dir, exist_ok=True)\n    with open(en_file, 'r', encoding='utf-8') as f_en, open(vi_file, 'r', encoding='utf-8') as f_vi:\n        en_lines = f_en.readlines()\n        vi_lines = f_vi.readlines()\n    \n    assert len(en_lines) == len(vi_lines), \"Number of lines don't match!\"\n    \n    data = [f\"[EN] {preprocess(en)} [VI] {preprocess(vi)}\" for en, vi in zip(en_lines, vi_lines)]\n    \n    train_data, temp_data = train_test_split(data, train_size=train_ratio, random_state=42)\n    val_data, test_data = train_test_split(temp_data, train_size=val_ratio/(val_ratio + 0.1), random_state=42)\n    \n    with open(os.path.join(output_dir, \"train_data.pkl\"), 'wb') as f:\n        pickle.dump(train_data, f)\n    with open(os.path.join(output_dir, \"val_data.pkl\"), 'wb') as f:\n        pickle.dump(val_data, f)\n    with open(os.path.join(output_dir, \"test_data.pkl\"), 'wb') as f:\n        pickle.dump(test_data, f)\n    \n    print(f\"Data saved: {len(train_data)} train, {len(val_data)} val, {len(test_data)} test\")\n    return train_data, val_data, test_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:57:58.917645Z","iopub.execute_input":"2025-05-18T08:57:58.918348Z","iopub.status.idle":"2025-05-18T08:57:58.926617Z","shell.execute_reply.started":"2025-05-18T08:57:58.918323Z","shell.execute_reply":"2025-05-18T08:57:58.925784Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# Run Step 2\nen_path = \"/kaggle/input/machinetranslation/train.en.txt\"\nvi_path = \"/kaggle/input/machinetranslation/train.vi.txt\"\ntrain_data, val_data, test_data = prepare_and_split_data(en_path, vi_path, output_dir=\"data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:58:25.414602Z","iopub.execute_input":"2025-05-18T08:58:25.415257Z","iopub.status.idle":"2025-05-18T08:58:28.630691Z","shell.execute_reply.started":"2025-05-18T08:58:25.415237Z","shell.execute_reply":"2025-05-18T08:58:28.629957Z"}},"outputs":[{"name":"stdout","text":"Data saved: 106653 train, 13332 val, 13332 test\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"# Step 3: Train SentencePiece Tokenizer\nimport sentencepiece as spm\n\ndef load_data_splits(output_dir=\"data\"):\n    with open(os.path.join(output_dir, \"train_data.pkl\"), 'rb') as f:\n        train_data = pickle.load(f)\n    with open(os.path.join(output_dir, \"val_data.pkl\"), 'rb') as f:\n        val_data = pickle.load(f)\n    with open(os.path.join(output_dir, \"test_data.pkl\"), 'rb') as f:\n        test_data = pickle.load(f)\n    return train_data, val_data, test_data\n\ndef train_tokenizer(corpus_lines, model_prefix=\"gpt_bpe\", vocab_size=8000, output_dir=\"tokenizer\"):\n    os.makedirs(output_dir, exist_ok=True)\n    corpus_file = os.path.join(output_dir, \"corpus.txt\")\n    with open(corpus_file, \"w\", encoding=\"utf-8\") as f:\n        for line in corpus_lines:\n            f.write(line + \"\\n\")\n    \n    spm.SentencePieceTrainer.train(\n        input=corpus_file,\n        model_prefix=os.path.join(output_dir, model_prefix),\n        vocab_size=vocab_size,\n        model_type=\"bpe\",\n        character_coverage=1.0,\n        bos_id=1,\n        eos_id=2,\n        pad_id=0,\n        unk_id=3\n    )\n    \n    sp = spm.SentencePieceProcessor()\n    sp.load(os.path.join(output_dir, f\"{model_prefix}.model\"))\n    print(f\"Tokenizer trained and saved to {output_dir}\")\n    return sp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T09:00:25.915307Z","iopub.execute_input":"2025-05-18T09:00:25.915715Z","iopub.status.idle":"2025-05-18T09:00:25.923201Z","shell.execute_reply.started":"2025-05-18T09:00:25.915682Z","shell.execute_reply":"2025-05-18T09:00:25.922183Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# Run Step 3\ntrain_data, val_data, test_data = load_data_splits(output_dir=\"data\")\nsp = train_tokenizer(train_data + val_data + test_data, model_prefix=\"gpt_bpe\", output_dir=\"tokenizer\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T09:00:28.656567Z","iopub.execute_input":"2025-05-18T09:00:28.656870Z","iopub.status.idle":"2025-05-18T09:00:35.804132Z","shell.execute_reply.started":"2025-05-18T09:00:28.656850Z","shell.execute_reply":"2025-05-18T09:00:35.803274Z"}},"outputs":[{"name":"stdout","text":"Tokenizer trained and saved to tokenizer\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"# Step 4: Create Encoded Datasets\nimport torch\nfrom torch.utils.data import Dataset\n\ndef encode_lines(lines, sp, max_len=128):\n    encoded = [[1] + sp.encode(line, out_type=int) + [2] for line in lines]\n    padded = [seq[:max_len] + [0] * (max_len - len(seq[:max_len])) for seq in encoded]\n    return torch.tensor(padded, dtype=torch.long), max_len\n\nclass TranslationDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n    def __getitem__(self, idx):\n        x = self.data[idx][:-1]\n        y = self.data[idx][1:]\n        return x, y\n    def __len__(self):\n        return len(self.data)\n\ndef create_datasets(train_data, val_data, test_data, sp, output_dir=\"data\", max_len=128):\n    os.makedirs(output_dir, exist_ok=True)\n    \n    train_enc, max_len = encode_lines(train_data, sp, max_len)\n    val_enc, _ = encode_lines(val_data, sp, max_len)\n    test_enc, _ = encode_lines(test_data, sp, max_len)\n    \n    train_dataset = TranslationDataset(train_enc)\n    val_dataset = TranslationDataset(val_enc)\n    test_dataset = TranslationDataset(test_enc)\n    \n    torch.save(train_enc, os.path.join(output_dir, \"train_enc.pt\"))\n    torch.save(val_enc, os.path.join(output_dir, \"val_enc.pt\"))\n    torch.save(test_enc, os.path.join(output_dir, \"test_enc.pt\"))\n    \n    with open(os.path.join(output_dir, \"max_len.pkl\"), 'wb') as f:\n        pickle.dump(max_len, f)\n    \n    print(f\"Datasets saved: {len(train_dataset)} train, {len(val_dataset)} val, {len(test_dataset)} test\")\n    return train_dataset, val_dataset, test_dataset, max_len","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T09:01:09.459102Z","iopub.execute_input":"2025-05-18T09:01:09.459972Z","iopub.status.idle":"2025-05-18T09:01:09.488122Z","shell.execute_reply.started":"2025-05-18T09:01:09.459947Z","shell.execute_reply":"2025-05-18T09:01:09.487297Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# Run Step 4\ntrain_dataset, val_dataset, test_dataset, max_len = create_datasets(\n    train_data, val_data, test_data, sp, output_dir=\"data\", max_len=128\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T09:01:15.401286Z","iopub.execute_input":"2025-05-18T09:01:15.401576Z","iopub.status.idle":"2025-05-18T09:01:39.238459Z","shell.execute_reply.started":"2025-05-18T09:01:15.401556Z","shell.execute_reply":"2025-05-18T09:01:39.237822Z"}},"outputs":[{"name":"stdout","text":"Datasets saved: 106653 train, 13332 val, 13332 test\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"# Step 5: Define GPT Model\nimport torch.nn as nn\nimport math\n\nimport torch\nimport torch.nn as nn\nimport math\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, nhead, dropout=0.1):\n        super().__init__()\n        assert d_model % nhead == 0\n        self.d_model = d_model\n        self.nhead = nhead\n        self.d_k = d_model // nhead\n        self.query = nn.Linear(d_model, d_model)\n        self.key = nn.Linear(d_model, d_model)\n        self.value = nn.Linear(d_model, d_model)\n        self.out = nn.Linear(d_model, d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.scale = math.sqrt(self.d_k)\n\n    def forward(self, x, mask=None):\n        batch_size = x.size(0)\n        q = self.query(x).view(batch_size, -1, self.nhead, self.d_k).transpose(1, 2)\n        k = self.key(x).view(batch_size, -1, self.nhead, self.d_k).transpose(1, 2)\n        v = self.value(x).view(batch_size, -1, self.nhead, self.d_k).transpose(1, 2)\n        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n        if mask is not None:\n            scores = scores.masked_fill(mask == float('-inf'), float('-inf'))\n        attn = torch.softmax(scores, dim=-1)\n        attn = self.dropout(attn)\n        context = torch.matmul(attn, v)\n        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n        return self.out(context)\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, dim_ff, dropout=0.1):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, dim_ff)\n        self.linear2 = nn.Linear(dim_ff, d_model)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.linear2(self.dropout(self.relu(self.linear1(x))))\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, dim_ff, dropout=0.1):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, nhead, dropout)\n        self.feed_forward = FeedForward(d_model, dim_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        x = self.norm1(x + self.dropout(self.self_attn(x, mask)))\n        x = self.norm2(x + self.dropout(self.feed_forward(x)))\n        return x\n\nclass GPTModel(nn.Module):\n    def __init__(self, vocab_size, d_model=256, nhead=8, num_layers=6, dim_ff=1024, max_len=128, dropout=0.1):\n        super().__init__()\n        self.token_embed = nn.Embedding(vocab_size, d_model)\n        self.pos_embed = nn.Parameter(self.create_pe(max_len, d_model), requires_grad=False)\n        self.layers = nn.ModuleList([\n            DecoderLayer(d_model, nhead, dim_ff, dropout) for _ in range(num_layers)\n        ])\n        self.fc = nn.Linear(d_model, vocab_size)\n        self.d_model = d_model\n        self.max_len = max_len\n        self.dropout = nn.Dropout(dropout)\n\n    def create_pe(self, max_len, d_model):\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        return pe.unsqueeze(0)\n\n    def forward(self, x):\n        batch_size, seq_len = x.size()\n        x = self.token_embed(x) * math.sqrt(self.d_model) + self.pos_embed[:, :seq_len, :].to(x.device)\n        x = self.dropout(x)\n        mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device) * float('-inf'), diagonal=1)\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.fc(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T09:06:31.714980Z","iopub.execute_input":"2025-05-18T09:06:31.715311Z","iopub.status.idle":"2025-05-18T09:06:31.736476Z","shell.execute_reply.started":"2025-05-18T09:06:31.715286Z","shell.execute_reply":"2025-05-18T09:06:31.735900Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"model = GPTModel(vocab_size=len(sp), max_len=max_len)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T09:06:37.236176Z","iopub.execute_input":"2025-05-18T09:06:37.236733Z","iopub.status.idle":"2025-05-18T09:06:37.321067Z","shell.execute_reply.started":"2025-05-18T09:06:37.236709Z","shell.execute_reply":"2025-05-18T09:06:37.320400Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport torch.optim as optim\nfrom torch.cuda.amp import GradScaler, autocast\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\ndef train(model, train_dataset, val_dataset, sp, max_len, test_data, output_dir=\"model\", epochs=15, lr=0.0001, patience=3):\n    os.makedirs(output_dir, exist_ok=True)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n    scaler = GradScaler()\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)\n\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, pin_memory=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, pin_memory=True, num_workers=2)\n\n    best_val_loss = float(\"inf\")\n    counter = 0\n    train_losses, val_losses = [], []\n\n    def translate_sample(input_text):\n        model.eval()\n        input_sentence = f\"[EN] {input_text.strip()} [VI]\"\n        input_ids = [1] + sp.encode(input_sentence, out_type=int)\n        input_ids = input_ids[:max_len-1] + [2]\n        input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)\n\n        with torch.no_grad():\n            for _ in range(max_len - len(input_ids)):\n                out = model(input_tensor)\n                next_token = torch.argmax(out[:, -1, :], dim=-1).item()\n                if next_token == 2:\n                    break\n                input_tensor = torch.cat([input_tensor, torch.tensor([[next_token]], device=device)], dim=1)\n\n        decoded = sp.decode(input_tensor[0].tolist())\n        return decoded.split(\"[VI]\")[-1].strip() if \"[VI]\" in decoded else decoded.strip()\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n            x, y = x.to(device), y.to(device)\n            optimizer.zero_grad()\n            with autocast():\n                out = model(x)\n                loss = criterion(out.view(-1, out.size(-1)), y.view(-1))\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(train_loader)\n        train_losses.append(avg_loss)\n\n        model.eval()\n        total_val_loss = 0\n        with torch.no_grad():\n            for x, y in val_loader:\n                x, y = x.to(device), y.to(device)\n                with autocast():\n                    out = model(x)\n                    loss = criterion(out.view(-1, out.size(-1)), y.view(-1))\n                total_val_loss += loss.item()\n        avg_val_loss = total_val_loss / len(val_loader)\n        val_losses.append(avg_val_loss)\n\n        print(f\"Epoch {epoch+1}, Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n        scheduler.step(avg_val_loss)\n\n        print(\"üß™ Sample Translations (5 sentences):\")\n        for idx in range(min(5, len(test_data))):\n            try:\n                item = test_data[idx]\n                input_text = item.split(\"[EN]\")[1].split(\"[VI]\")[0].strip()\n                ref_text = item.split(\"[VI]\")[-1].strip()\n                pred_text = translate_sample(input_text)\n                print(f\"[{idx+1}]\")\n                print(f\"EN   : {input_text}\")\n                print(f\"REF  : {ref_text}\")\n                print(f\"PRED : {pred_text}\")\n                print(\"-\" * 50)\n            except Exception as e:\n                print(f\"‚ö†Ô∏è Error at sample {idx}: {e}\")\n\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            counter = 0\n            torch.save(model.state_dict(), os.path.join(output_dir, \"best_gpt_model.pth\"))\n            print(\"‚úÖ Model saved!\")\n        else:\n            counter += 1\n            if counter >= patience:\n                print(\"‚õî Early stopping triggered.\")\n                break\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(train_losses, label=\"Train Loss\", marker='o')\n    plt.plot(val_losses, label=\"Validation Loss\", marker='x')\n    plt.title(\"Training vs Validation Loss per Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid(True)\n    plt.savefig(os.path.join(output_dir, \"loss_plot.png\"))\n    plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T10:06:21.679206Z","iopub.execute_input":"2025-05-18T10:06:21.679975Z","iopub.status.idle":"2025-05-18T10:06:21.698701Z","shell.execute_reply.started":"2025-05-18T10:06:21.679947Z","shell.execute_reply":"2025-05-18T10:06:21.698036Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"# Run Step 6\ntrain(model, train_dataset, val_dataset, sp, max_len, test_data, output_dir=\"model\", epochs=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T11:21:45.784742Z","iopub.execute_input":"2025-05-18T11:21:45.785113Z","iopub.status.idle":"2025-05-18T13:43:31.583267Z","shell.execute_reply.started":"2025-05-18T11:21:45.785080Z","shell.execute_reply":"2025-05-18T13:43:31.582170Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/2019205525.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\nEpoch 1 Training:   0%|          | 0/13332 [00:00<?, ?it/s]/tmp/ipykernel_35/2019205525.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nEpoch 1 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13332/13332 [06:45<00:00, 32.89it/s]\n/tmp/ipykernel_35/2019205525.py:63: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Train Loss: 2.8485, Val Loss: 2.8657\nüß™ Sample Translations (5 sentences):\n[1]\nEN   : And I would ask him every once in a while , \" Dad , do you want it to sound like the record ? \"\nREF  : ƒê√¥i khi , t√¥i h·ªèi √¥ng r·∫±ng : \" B·ªë ∆°i , b·ªë c√≥ mu·ªën ch∆°i gi·ªëng nh∆∞ b·∫£n g·ªëc kh√¥ng ·∫° ? \"\nPRED : n√≥i v·ªõi √¥ng ·∫•y m·ªói l·∫ßn , \" Dad , b·∫°n mu·ªën nghe √¢m thanh nh∆∞ ghi √¢m nh·∫°c v·∫≠y ? \"\n--------------------------------------------------\n[2]\nEN   : So , we 're only going to look at the stars inside that small square , although we 've looked at all of them .\nREF  : V·∫≠y , ch√∫ng ta s·∫Ω ch·ªâ nh√¨n v√†o nh·ªØng ng√¥i sao b√™n trong h√¨nh vu√¥ng nh·ªè ƒë√≥ , m·∫∑t d√π ch√∫ng ta ƒë√£ nh√¨n t·∫•t c·∫£ ch√∫ng .\nPRED : ch√∫ng ta ch·ªâ nh√¨n v√†o c√°c ng√¥i sao vu√¥ng trong vu√¥ng nh·ªè , m·∫∑c d√π ch√∫ng ta nh√¨n v√†o t·∫•t c·∫£ ch√∫ng .\n--------------------------------------------------\n[3]\nEN   : And they travel out into the cosmos at the speed of light .\nREF  : V√† ch√∫ng ƒëi v√†o v≈© tr·ª• v·ªõi v·∫≠n t·ªëc √°nh s√°ng .\nPRED : ch√∫ng ƒëi v√†o t·ªëc ƒë·ªô √°nh s√°ng c·ªßa √°nh s√°ng .\n--------------------------------------------------\n[4]\nEN   : Whatever it is , if it 's really important , you owe it to yourself to look at this toolbox and the engine that it 's going to work on , and no engine works well without being warmed up .\nREF  : S·ª± ki·ªán n√†o ƒëi n·ªØa , n·∫øu n√≥ r·∫•t quan tr·ªçng , b·∫°n t·ª± th·∫•y m√¨nh c·∫ßn ƒë·∫øn c√¥ng c·ª• n√†y v√† c·ªó m√°y s·∫Ω ƒë∆∞·ª£c v·∫∑n l√™n , v√† kh√¥ng m√°y n√†o ch·∫°y t·ªët n·∫øu kh√¥ng ƒë∆∞·ª£c l√†m ·∫•m .\nPRED : n·∫øu n√≥ th·∫≠t s·ª± quan tr·ªçng , b·∫°n s·∫Ω t·ª± nh·ªßa v√†o c√¥ng c·ª• n√†y v√† ƒë·ªông c∆° m√† n√≥ s·∫Ω l√†m vi·ªác , v√† kh√¥ng c√≥ ƒë·ªông c∆° kh√¥ng ho·∫°t ƒë·ªông n√†o , v√† kh√¥ng c√≥ ƒë·ªông c∆° th·ªÉ n√†o kh√¥ng c√≥ chi·∫øn tranh .\n--------------------------------------------------\n[5]\nEN   : So , my message to you folks , from my three minutes , is that I appreciate the chance to share this with you . I will be back . I love being at TED .\nREF  : th√¥ng ƒëi·ªáp c·ªßa t√¥i mu·ªën g·ª≠i t·ªõi m·ªçi ng∆∞·ªùi trong 3 ph√∫t ·ªü ƒë√¢y c·ªßa t√¥i , ƒë√≥ l√† t√¥i tr√¢n tr·ªçng c∆° h·ªôi ƒë∆∞·ª£c chia s·∫ª c√πng c√°c b·∫°n . T√¥i s·∫Ω quay tr·ªü l·∫°i . T√¥i th√≠ch tham gia TED .\nPRED : ƒëi·ªáp c·ªßa t√¥i v·ªõi b·∫°n , t·ª´ ba ph√∫t , t√¥i l√† t√¥i r·∫•t tr√¢n tr·ªçng c∆° h·ªôi ƒë·ªÉ chia s·∫ª v·ªõi c√°c b·∫°n . T√¥i y√™u qu√Ω v·ªã s·∫Ω tr·ªü l·∫°i . T√¥i y√™u th√≠ch ·ªü TED .\n--------------------------------------------------\n‚úÖ Model saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13332/13332 [06:48<00:00, 32.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Train Loss: 2.8198, Val Loss: 2.8489\nüß™ Sample Translations (5 sentences):\n[1]\nEN   : And I would ask him every once in a while , \" Dad , do you want it to sound like the record ? \"\nREF  : ƒê√¥i khi , t√¥i h·ªèi √¥ng r·∫±ng : \" B·ªë ∆°i , b·ªë c√≥ mu·ªën ch∆°i gi·ªëng nh∆∞ b·∫£n g·ªëc kh√¥ng ·∫° ? \"\nPRED : h·ªèi √¥ng ·∫•y m·ªói l·∫ßn trong m·ªôt l√∫c , \" Dad , b·∫°n mu·ªën n√≥ nghe nh∆∞ ghi √¢m thanh ? \"\n--------------------------------------------------\n[2]\nEN   : So , we 're only going to look at the stars inside that small square , although we 've looked at all of them .\nREF  : V·∫≠y , ch√∫ng ta s·∫Ω ch·ªâ nh√¨n v√†o nh·ªØng ng√¥i sao b√™n trong h√¨nh vu√¥ng nh·ªè ƒë√≥ , m·∫∑t d√π ch√∫ng ta ƒë√£ nh√¨n t·∫•t c·∫£ ch√∫ng .\nPRED : ch√∫ng ta ch·ªâ nh√¨n v√†o c√°c ng√¥i sao vu√¥ng trong vu√¥ng nh·ªè , m·∫∑c d√π ch√∫ng ta nh√¨n v√†o t·∫•t c·∫£ ch√∫ng .\n--------------------------------------------------\n[3]\nEN   : And they travel out into the cosmos at the speed of light .\nREF  : V√† ch√∫ng ƒëi v√†o v≈© tr·ª• v·ªõi v·∫≠n t·ªëc √°nh s√°ng .\nPRED : ch√∫ng ƒëi v√†o t·ªëc ƒë·ªô √°nh s√°ng c·ªßa √°nh s√°ng .\n--------------------------------------------------\n[4]\nEN   : Whatever it is , if it 's really important , you owe it to yourself to look at this toolbox and the engine that it 's going to work on , and no engine works well without being warmed up .\nREF  : S·ª± ki·ªán n√†o ƒëi n·ªØa , n·∫øu n√≥ r·∫•t quan tr·ªçng , b·∫°n t·ª± th·∫•y m√¨nh c·∫ßn ƒë·∫øn c√¥ng c·ª• n√†y v√† c·ªó m√°y s·∫Ω ƒë∆∞·ª£c v·∫∑n l√™n , v√† kh√¥ng m√°y n√†o ch·∫°y t·ªët n·∫øu kh√¥ng ƒë∆∞·ª£c l√†m ·∫•m .\nPRED : n√≥ l√† , n·∫øu n√≥ th·∫≠t s·ª± quan tr·ªçng , b·∫°n s·∫Ω t·ª± nh√©t n√≥ v√†o c√¥ng c·ª• n√†y v√† ƒë·ªông c∆° m√† n√≥ s·∫Ω l√†m vi·ªác , v√† kh√¥ng c√≥ ƒë·ªông c∆° v·∫≠n ƒë·ªông n√†o m√† kh√¥ng c√≥ chi·∫øn tranh .\n--------------------------------------------------\n[5]\nEN   : So , my message to you folks , from my three minutes , is that I appreciate the chance to share this with you . I will be back . I love being at TED .\nREF  : th√¥ng ƒëi·ªáp c·ªßa t√¥i mu·ªën g·ª≠i t·ªõi m·ªçi ng∆∞·ªùi trong 3 ph√∫t ·ªü ƒë√¢y c·ªßa t√¥i , ƒë√≥ l√† t√¥i tr√¢n tr·ªçng c∆° h·ªôi ƒë∆∞·ª£c chia s·∫ª c√πng c√°c b·∫°n . T√¥i s·∫Ω quay tr·ªü l·∫°i . T√¥i th√≠ch tham gia TED .\nPRED : g·ª≠i l·ªùi nh·∫Øn c·ªßa t√¥i v·ªõi c√°c b·∫°n , t·ª´ ba ph√∫t , t√¥i nh·∫≠n ra r·∫±ng t√¥i c√≥ c∆° h·ªôi ƒë·ªÉ chia s·∫ª v·ªõi c√°c b·∫°n . T√¥i r·∫•t th√≠ch th√∫ khi ·ªü TED .\n--------------------------------------------------\n‚úÖ Model saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13332/13332 [06:51<00:00, 32.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Train Loss: 2.7992, Val Loss: 2.8371\nüß™ Sample Translations (5 sentences):\n[1]\nEN   : And I would ask him every once in a while , \" Dad , do you want it to sound like the record ? \"\nREF  : ƒê√¥i khi , t√¥i h·ªèi √¥ng r·∫±ng : \" B·ªë ∆°i , b·ªë c√≥ mu·ªën ch∆°i gi·ªëng nh∆∞ b·∫£n g·ªëc kh√¥ng ·∫° ? \"\nPRED : t√¥i s·∫Ω h·ªèi √¥ng ta m·ªói l·∫ßn trong m·ªôt l√∫c , \" Dad , b·∫°n mu·ªën n√≥ nghe nh∆∞ l√† ghi √¢m ? \"\n--------------------------------------------------\n[2]\nEN   : So , we 're only going to look at the stars inside that small square , although we 've looked at all of them .\nREF  : V·∫≠y , ch√∫ng ta s·∫Ω ch·ªâ nh√¨n v√†o nh·ªØng ng√¥i sao b√™n trong h√¨nh vu√¥ng nh·ªè ƒë√≥ , m·∫∑t d√π ch√∫ng ta ƒë√£ nh√¨n t·∫•t c·∫£ ch√∫ng .\nPRED : d·ª• ch√∫ng ta ch·ªâ nh√¨n v√†o c√°c ng√¥i sao nh·ªè b√© x√≠u , m·∫∑c d√π ch√∫ng ta nh√¨n v√†o t·∫•t c·∫£ ch√∫ng .\n--------------------------------------------------\n[3]\nEN   : And they travel out into the cosmos at the speed of light .\nREF  : V√† ch√∫ng ƒëi v√†o v≈© tr·ª• v·ªõi v·∫≠n t·ªëc √°nh s√°ng .\nPRED : ch√∫ng ƒëi v√†o ƒë∆∞·ªùng bay c·ªßa √°nh s√°ng .\n--------------------------------------------------\n[4]\nEN   : Whatever it is , if it 's really important , you owe it to yourself to look at this toolbox and the engine that it 's going to work on , and no engine works well without being warmed up .\nREF  : S·ª± ki·ªán n√†o ƒëi n·ªØa , n·∫øu n√≥ r·∫•t quan tr·ªçng , b·∫°n t·ª± th·∫•y m√¨nh c·∫ßn ƒë·∫øn c√¥ng c·ª• n√†y v√† c·ªó m√°y s·∫Ω ƒë∆∞·ª£c v·∫∑n l√™n , v√† kh√¥ng m√°y n√†o ch·∫°y t·ªët n·∫øu kh√¥ng ƒë∆∞·ª£c l√†m ·∫•m .\nPRED : n·∫øu n√≥ th·∫≠t s·ª± quan tr·ªçng , b·∫°n s·∫Ω th·∫≠t s·ª± quan tr·ªçng , b·∫°n s·∫Ω t·ª± nh·ªß n√≥ v√†o c√¥ng c·ª• v√† ƒë·ªông c∆° ƒë√≥ s·∫Ω kh√¥ng c√≥ t√°c d·ª•ng g√¨ , v√† kh√¥ng c√≥ ƒë·ªông c∆° v·∫≠n ƒë·ªông g√¨ .\n--------------------------------------------------\n[5]\nEN   : So , my message to you folks , from my three minutes , is that I appreciate the chance to share this with you . I will be back . I love being at TED .\nREF  : th√¥ng ƒëi·ªáp c·ªßa t√¥i mu·ªën g·ª≠i t·ªõi m·ªçi ng∆∞·ªùi trong 3 ph√∫t ·ªü ƒë√¢y c·ªßa t√¥i , ƒë√≥ l√† t√¥i tr√¢n tr·ªçng c∆° h·ªôi ƒë∆∞·ª£c chia s·∫ª c√πng c√°c b·∫°n . T√¥i s·∫Ω quay tr·ªü l·∫°i . T√¥i th√≠ch tham gia TED .\nPRED : g·ª≠i tin nh·∫Øn c·ªßa t√¥i v·ªõi c√°c b·∫°n , t·ª´ ba ph√∫t , t√¥i r·∫•t tr√¢n tr·ªçng c∆° h·ªôi ƒë·ªÉ chia s·∫ª v·ªõi c√°c b·∫°n . T√¥i th√≠ch th√∫ v·ªõi c√°c b·∫°n . T√¥i y√™u ·ªü TED .\n--------------------------------------------------\n‚úÖ Model saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13332/13332 [06:48<00:00, 32.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Train Loss: 2.7789, Val Loss: 2.8275\nüß™ Sample Translations (5 sentences):\n[1]\nEN   : And I would ask him every once in a while , \" Dad , do you want it to sound like the record ? \"\nREF  : ƒê√¥i khi , t√¥i h·ªèi √¥ng r·∫±ng : \" B·ªë ∆°i , b·ªë c√≥ mu·ªën ch∆°i gi·ªëng nh∆∞ b·∫£n g·ªëc kh√¥ng ·∫° ? \"\nPRED : t√¥i h·ªèi √¥ng t·ª´ng l·∫ßn trong m·ªôt l√∫c , \" Dad , anh mu·ªën n√≥ nghe nh∆∞ b·∫£n ghi √¢m nh·∫°c ? \"\n--------------------------------------------------\n[2]\nEN   : So , we 're only going to look at the stars inside that small square , although we 've looked at all of them .\nREF  : V·∫≠y , ch√∫ng ta s·∫Ω ch·ªâ nh√¨n v√†o nh·ªØng ng√¥i sao b√™n trong h√¨nh vu√¥ng nh·ªè ƒë√≥ , m·∫∑t d√π ch√∫ng ta ƒë√£ nh√¨n t·∫•t c·∫£ ch√∫ng .\nPRED : d·ª• ch√∫ng ta ch·ªâ nh√¨n v√†o c√°c ng√¥i sao vu√¥ng nh·ªè , m·∫∑c d√π ch√∫ng ta nh√¨n v√†o t·∫•t c·∫£ ch√∫ng .\n--------------------------------------------------\n[3]\nEN   : And they travel out into the cosmos at the speed of light .\nREF  : V√† ch√∫ng ƒëi v√†o v≈© tr·ª• v·ªõi v·∫≠n t·ªëc √°nh s√°ng .\nPRED : ch√∫ng ƒëi v√†o t·ªëc ƒë·ªô c·ªßa v≈© tr·ª• .\n--------------------------------------------------\n[4]\nEN   : Whatever it is , if it 's really important , you owe it to yourself to look at this toolbox and the engine that it 's going to work on , and no engine works well without being warmed up .\nREF  : S·ª± ki·ªán n√†o ƒëi n·ªØa , n·∫øu n√≥ r·∫•t quan tr·ªçng , b·∫°n t·ª± th·∫•y m√¨nh c·∫ßn ƒë·∫øn c√¥ng c·ª• n√†y v√† c·ªó m√°y s·∫Ω ƒë∆∞·ª£c v·∫∑n l√™n , v√† kh√¥ng m√°y n√†o ch·∫°y t·ªët n·∫øu kh√¥ng ƒë∆∞·ª£c l√†m ·∫•m .\nPRED : n·∫øu n√≥ th·∫≠t s·ª± quan tr·ªçng , b·∫°n s·∫Ω l√†m cho b·∫£n th√¢n m√¨nh tr√¥ng n√≥ v√†o d·ª•ng c·ª• v√† ƒë·ªông c∆° ƒë√≥ s·∫Ω l√†m vi·ªác tr√™n chi·∫øc h·ªôp n√†y , v√† kh√¥ng c√≥ ƒë·ªông c∆° s·∫Ω kh√¥ng c√≥ chi·∫øn tranh n√†o .\n--------------------------------------------------\n[5]\nEN   : So , my message to you folks , from my three minutes , is that I appreciate the chance to share this with you . I will be back . I love being at TED .\nREF  : th√¥ng ƒëi·ªáp c·ªßa t√¥i mu·ªën g·ª≠i t·ªõi m·ªçi ng∆∞·ªùi trong 3 ph√∫t ·ªü ƒë√¢y c·ªßa t√¥i , ƒë√≥ l√† t√¥i tr√¢n tr·ªçng c∆° h·ªôi ƒë∆∞·ª£c chia s·∫ª c√πng c√°c b·∫°n . T√¥i s·∫Ω quay tr·ªü l·∫°i . T√¥i th√≠ch tham gia TED .\nPRED : tin nh·∫Øn c·ªßa t√¥i v·ªõi b·∫°n , t·ª´ ba ph√∫t , t√¥i l√† t√¥i r·∫•t tr√¢n tr·ªçng c∆° h·ªôi ƒë·ªÉ chia s·∫ª v·ªõi b·∫°n . T√¥i s·∫Ω tr·ªü l·∫°i v·ªõi b·∫°n . T√¥i y√™u th√≠ch th√∫ t·∫°i TED .\n--------------------------------------------------\n‚úÖ Model saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13332/13332 [06:49<00:00, 32.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Train Loss: 2.7610, Val Loss: 2.8139\nüß™ Sample Translations (5 sentences):\n[1]\nEN   : And I would ask him every once in a while , \" Dad , do you want it to sound like the record ? \"\nREF  : ƒê√¥i khi , t√¥i h·ªèi √¥ng r·∫±ng : \" B·ªë ∆°i , b·ªë c√≥ mu·ªën ch∆°i gi·ªëng nh∆∞ b·∫£n g·ªëc kh√¥ng ·∫° ? \"\nPRED : t√¥i h·ªèi anh ta m·ªói l·∫ßn trong m·ªôt th·ªùi gian , \" Dad , b·∫°n mu·ªën n√≥ nghe nh∆∞ ghi √¢m nh·∫°c ? \"\n--------------------------------------------------\n[2]\nEN   : So , we 're only going to look at the stars inside that small square , although we 've looked at all of them .\nREF  : V·∫≠y , ch√∫ng ta s·∫Ω ch·ªâ nh√¨n v√†o nh·ªØng ng√¥i sao b√™n trong h√¨nh vu√¥ng nh·ªè ƒë√≥ , m·∫∑t d√π ch√∫ng ta ƒë√£ nh√¨n t·∫•t c·∫£ ch√∫ng .\nPRED : ch√∫ng ta ch·ªâ nh√¨n v√†o c√°c ng√¥i sao nh·ªè trong h√¨nh vu√¥ng nh·ªè , m·∫∑c d√π ch√∫ng ta nh√¨n v√†o t·∫•t c·∫£ ch√∫ng .\n--------------------------------------------------\n[3]\nEN   : And they travel out into the cosmos at the speed of light .\nREF  : V√† ch√∫ng ƒëi v√†o v≈© tr·ª• v·ªõi v·∫≠n t·ªëc √°nh s√°ng .\nPRED : ch√∫ng ƒëi v√†o t·ªëc ƒë·ªô √°nh s√°ng c·ªßa √°nh s√°ng .\n--------------------------------------------------\n[4]\nEN   : Whatever it is , if it 's really important , you owe it to yourself to look at this toolbox and the engine that it 's going to work on , and no engine works well without being warmed up .\nREF  : S·ª± ki·ªán n√†o ƒëi n·ªØa , n·∫øu n√≥ r·∫•t quan tr·ªçng , b·∫°n t·ª± th·∫•y m√¨nh c·∫ßn ƒë·∫øn c√¥ng c·ª• n√†y v√† c·ªó m√°y s·∫Ω ƒë∆∞·ª£c v·∫∑n l√™n , v√† kh√¥ng m√°y n√†o ch·∫°y t·ªët n·∫øu kh√¥ng ƒë∆∞·ª£c l√†m ·∫•m .\nPRED : chƒÉng n·ªØa , n·∫øu n√≥ th·ª±c s·ª± quan tr·ªçng , b·∫°n s·∫Ω t·ª± nh√©t n√≥ ƒë·ªÉ nh√¨n v√†o c√¥ng c·ª• n√†y v√† ƒë·ªông c∆° m√† n√≥ s·∫Ω ho·∫°t ƒë·ªông v√†o , v√† kh√¥ng c√≥ ƒë·ªông c∆° v·∫≠n ƒë·ªông n√†o m√† kh√¥ng c√≥ chi·∫øn tranh .\n--------------------------------------------------\n[5]\nEN   : So , my message to you folks , from my three minutes , is that I appreciate the chance to share this with you . I will be back . I love being at TED .\nREF  : th√¥ng ƒëi·ªáp c·ªßa t√¥i mu·ªën g·ª≠i t·ªõi m·ªçi ng∆∞·ªùi trong 3 ph√∫t ·ªü ƒë√¢y c·ªßa t√¥i , ƒë√≥ l√† t√¥i tr√¢n tr·ªçng c∆° h·ªôi ƒë∆∞·ª£c chia s·∫ª c√πng c√°c b·∫°n . T√¥i s·∫Ω quay tr·ªü l·∫°i . T√¥i th√≠ch tham gia TED .\nPRED : tin nh·∫Øn c·ªßa t√¥i v·ªõi c√°c b·∫°n , t·ª´ 3 ph√∫t t√¥i , l√† t√¥i ƒë√°nh gi√° c∆° h·ªôi ƒë·ªÉ chia s·∫ª v·ªõi c√°c b·∫°n . T√¥i s·∫Ω tr·ªü l·∫°i v·ªõi TED . T√¥i r·∫•t th√≠ch th√∫ khi ·ªü TED .\n--------------------------------------------------\n‚úÖ Model saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13332/13332 [06:48<00:00, 32.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Train Loss: 2.7456, Val Loss: 2.8067\nüß™ Sample Translations (5 sentences):\n[1]\nEN   : And I would ask him every once in a while , \" Dad , do you want it to sound like the record ? \"\nREF  : ƒê√¥i khi , t√¥i h·ªèi √¥ng r·∫±ng : \" B·ªë ∆°i , b·ªë c√≥ mu·ªën ch∆°i gi·ªëng nh∆∞ b·∫£n g·ªëc kh√¥ng ·∫° ? \"\nPRED : t√¥i h·ªèi √¥ng t·ª´ng c√≥ m·ªôt l·∫ßn trong m·ªôt l·∫ßn , \" Dad , b·∫°n mu·ªën n√≥ nghe nh∆∞ ghi √¢m nh·∫°c ? \"\n--------------------------------------------------\n[2]\nEN   : So , we 're only going to look at the stars inside that small square , although we 've looked at all of them .\nREF  : V·∫≠y , ch√∫ng ta s·∫Ω ch·ªâ nh√¨n v√†o nh·ªØng ng√¥i sao b√™n trong h√¨nh vu√¥ng nh·ªè ƒë√≥ , m·∫∑t d√π ch√∫ng ta ƒë√£ nh√¨n t·∫•t c·∫£ ch√∫ng .\nPRED : ch√∫ng ta ch·ªâ nh√¨n v√†o c√°c ng√¥i sao tr√™n h√¨nh vu√¥ng nh·ªè , m·∫∑c d√π ch√∫ng ta nh√¨n v√†o t·∫•t c·∫£ ch√∫ng .\n--------------------------------------------------\n[3]\nEN   : And they travel out into the cosmos at the speed of light .\nREF  : V√† ch√∫ng ƒëi v√†o v≈© tr·ª• v·ªõi v·∫≠n t·ªëc √°nh s√°ng .\nPRED : ch√∫ng ƒëi v√†o t·ªëc ƒë·ªô √°nh s√°ng c·ªßa √°nh s√°ng .\n--------------------------------------------------\n[4]\nEN   : Whatever it is , if it 's really important , you owe it to yourself to look at this toolbox and the engine that it 's going to work on , and no engine works well without being warmed up .\nREF  : S·ª± ki·ªán n√†o ƒëi n·ªØa , n·∫øu n√≥ r·∫•t quan tr·ªçng , b·∫°n t·ª± th·∫•y m√¨nh c·∫ßn ƒë·∫øn c√¥ng c·ª• n√†y v√† c·ªó m√°y s·∫Ω ƒë∆∞·ª£c v·∫∑n l√™n , v√† kh√¥ng m√°y n√†o ch·∫°y t·ªët n·∫øu kh√¥ng ƒë∆∞·ª£c l√†m ·∫•m .\nPRED : d·ª• , n·∫øu n√≥ th·ª±c s·ª± quan tr·ªçng , b·∫°n s·∫Ω t·ª± nh·ªß n√≥ ƒë·ªÉ nh√¨n v√†o h·ªôp n√†y v√† ƒë·ªông c∆° m√† n√≥ s·∫Ω ho·∫°t ƒë·ªông , v√† kh√¥ng c√≥ ƒë·ªông c∆° t·ªët m√† kh√¥ng c·∫ßn ph·∫£i l√† chi·∫øn tranh .\n--------------------------------------------------\n[5]\nEN   : So , my message to you folks , from my three minutes , is that I appreciate the chance to share this with you . I will be back . I love being at TED .\nREF  : th√¥ng ƒëi·ªáp c·ªßa t√¥i mu·ªën g·ª≠i t·ªõi m·ªçi ng∆∞·ªùi trong 3 ph√∫t ·ªü ƒë√¢y c·ªßa t√¥i , ƒë√≥ l√† t√¥i tr√¢n tr·ªçng c∆° h·ªôi ƒë∆∞·ª£c chia s·∫ª c√πng c√°c b·∫°n . T√¥i s·∫Ω quay tr·ªü l·∫°i . T√¥i th√≠ch tham gia TED .\nPRED : tin nh·∫Øn c·ªßa t√¥i v·ªõi c√°c b·∫°n , t·ª´ 3 ph√∫t , t√¥i l√† t√¥i ƒë√°nh gi√° c∆° h·ªôi ƒë·ªÉ chia s·∫ª v·ªõi c√°c b·∫°n . T√¥i s·∫Ω y√™u qu√Ω v·ªã ·ªü TED . T√¥i y√™u th√≠ch ·ªü TED .\n--------------------------------------------------\n‚úÖ Model saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13332/13332 [06:49<00:00, 32.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Train Loss: 2.7307, Val Loss: 2.7992\nüß™ Sample Translations (5 sentences):\n[1]\nEN   : And I would ask him every once in a while , \" Dad , do you want it to sound like the record ? \"\nREF  : ƒê√¥i khi , t√¥i h·ªèi √¥ng r·∫±ng : \" B·ªë ∆°i , b·ªë c√≥ mu·ªën ch∆°i gi·ªëng nh∆∞ b·∫£n g·ªëc kh√¥ng ·∫° ? \"\nPRED : t√¥i h·ªèi √¥ng m·ªói l·∫ßn , \" Dad , anh mu·ªën n√≥ nghe √¢m thanh nh∆∞ ghi √¢m ? \"\n--------------------------------------------------\n[2]\nEN   : So , we 're only going to look at the stars inside that small square , although we 've looked at all of them .\nREF  : V·∫≠y , ch√∫ng ta s·∫Ω ch·ªâ nh√¨n v√†o nh·ªØng ng√¥i sao b√™n trong h√¨nh vu√¥ng nh·ªè ƒë√≥ , m·∫∑t d√π ch√∫ng ta ƒë√£ nh√¨n t·∫•t c·∫£ ch√∫ng .\nPRED : ch√∫ng ta ch·ªâ nh√¨n nh·ªØng ng√¥i sao trong h√¨nh vu√¥ng nh·ªè , m·∫∑c d√π ch√∫ng ta nh√¨n v√†o t·∫•t c·∫£ ch√∫ng .\n--------------------------------------------------\n[3]\nEN   : And they travel out into the cosmos at the speed of light .\nREF  : V√† ch√∫ng ƒëi v√†o v≈© tr·ª• v·ªõi v·∫≠n t·ªëc √°nh s√°ng .\nPRED : ch√∫ng ƒëi v√†o t·ªëc ƒë·ªô √°nh s√°ng .\n--------------------------------------------------\n[4]\nEN   : Whatever it is , if it 's really important , you owe it to yourself to look at this toolbox and the engine that it 's going to work on , and no engine works well without being warmed up .\nREF  : S·ª± ki·ªán n√†o ƒëi n·ªØa , n·∫øu n√≥ r·∫•t quan tr·ªçng , b·∫°n t·ª± th·∫•y m√¨nh c·∫ßn ƒë·∫øn c√¥ng c·ª• n√†y v√† c·ªó m√°y s·∫Ω ƒë∆∞·ª£c v·∫∑n l√™n , v√† kh√¥ng m√°y n√†o ch·∫°y t·ªët n·∫øu kh√¥ng ƒë∆∞·ª£c l√†m ·∫•m .\nPRED : chƒÉng n·ªØa , n·∫øu n√≥ th·ª±c s·ª± quan tr·ªçng , b·∫°n s·∫Ω t·ª± nh√©t n√≥ v√†o c√¥ng c·ª• n√†y v√† ƒë·ªông c∆° m√† n√≥ s·∫Ω ho·∫°t ƒë·ªông v√†o c√¥ng c·ª• v√† ƒë·ªông c∆° m√† kh√¥ng c√≥ chi·∫øn tranh g√¨ .\n--------------------------------------------------\n[5]\nEN   : So , my message to you folks , from my three minutes , is that I appreciate the chance to share this with you . I will be back . I love being at TED .\nREF  : th√¥ng ƒëi·ªáp c·ªßa t√¥i mu·ªën g·ª≠i t·ªõi m·ªçi ng∆∞·ªùi trong 3 ph√∫t ·ªü ƒë√¢y c·ªßa t√¥i , ƒë√≥ l√† t√¥i tr√¢n tr·ªçng c∆° h·ªôi ƒë∆∞·ª£c chia s·∫ª c√πng c√°c b·∫°n . T√¥i s·∫Ω quay tr·ªü l·∫°i . T√¥i th√≠ch tham gia TED .\nPRED : th√¥ng ƒëi·ªáp c·ªßa t√¥i v·ªõi c√°c b·∫°n , t·ª´ ba ph√∫t , t√¥i nh·∫≠n ra c∆° h·ªôi ƒë·ªÉ chia s·∫ª v·ªõi c√°c b·∫°n . T√¥i s·∫Ω tr·ªü l·∫°i v·ªõi c√°c b·∫°n . T√¥i y√™u th√≠ch ·ªü TED .\n--------------------------------------------------\n‚úÖ Model saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13332/13332 [06:46<00:00, 32.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Train Loss: 2.7161, Val Loss: 2.7928\nüß™ Sample Translations (5 sentences):\n[1]\nEN   : And I would ask him every once in a while , \" Dad , do you want it to sound like the record ? \"\nREF  : ƒê√¥i khi , t√¥i h·ªèi √¥ng r·∫±ng : \" B·ªë ∆°i , b·ªë c√≥ mu·ªën ch∆°i gi·ªëng nh∆∞ b·∫£n g·ªëc kh√¥ng ·∫° ? \"\nPRED : h·ªèi anh ta m·ªói l·∫ßn trong m·ªôt l√∫c , \" Dad , anh mu·ªën n√≥ nghe nh∆∞ l√† ghi √¢m ? \"\n--------------------------------------------------\n[2]\nEN   : So , we 're only going to look at the stars inside that small square , although we 've looked at all of them .\nREF  : V·∫≠y , ch√∫ng ta s·∫Ω ch·ªâ nh√¨n v√†o nh·ªØng ng√¥i sao b√™n trong h√¨nh vu√¥ng nh·ªè ƒë√≥ , m·∫∑t d√π ch√∫ng ta ƒë√£ nh√¨n t·∫•t c·∫£ ch√∫ng .\nPRED : ch√∫ng ta ch·ªâ nh√¨n v√†o c√°c ng√¥i sao trong h√¨nh vu√¥ng nh·ªè b√© , m·∫∑c d√π ch√∫ng ta nh√¨n v√†o t·∫•t c·∫£ ch√∫ng .\n--------------------------------------------------\n[3]\nEN   : And they travel out into the cosmos at the speed of light .\nREF  : V√† ch√∫ng ƒëi v√†o v≈© tr·ª• v·ªõi v·∫≠n t·ªëc √°nh s√°ng .\nPRED : ch√∫ng ƒëi v√†o t·ªëc ƒë·ªô v≈© tr·ª• t·ªëc ƒë·ªô √°nh s√°ng .\n--------------------------------------------------\n[4]\nEN   : Whatever it is , if it 's really important , you owe it to yourself to look at this toolbox and the engine that it 's going to work on , and no engine works well without being warmed up .\nREF  : S·ª± ki·ªán n√†o ƒëi n·ªØa , n·∫øu n√≥ r·∫•t quan tr·ªçng , b·∫°n t·ª± th·∫•y m√¨nh c·∫ßn ƒë·∫øn c√¥ng c·ª• n√†y v√† c·ªó m√°y s·∫Ω ƒë∆∞·ª£c v·∫∑n l√™n , v√† kh√¥ng m√°y n√†o ch·∫°y t·ªët n·∫øu kh√¥ng ƒë∆∞·ª£c l√†m ·∫•m .\nPRED : chƒÉng n·ªØa , n·∫øu n√≥ th·∫≠t s·ª± quan tr·ªçng , b·∫°n s·∫Ω t·ª± nh·ªß n√≥ v√†o d·ª•ng c·ª• n√†y v√† ƒë·ªông c∆° m√† n√≥ s·∫Ω ho·∫°t ƒë·ªông nh∆∞ l√† chi·∫øn tranh v√† kh√¥ng c√≥ ƒë·ªông c∆° s·ªü h·∫° c√°nh tay n√†o m√† kh√¥ng c√≥ chi·∫øn tranh .\n--------------------------------------------------\n[5]\nEN   : So , my message to you folks , from my three minutes , is that I appreciate the chance to share this with you . I will be back . I love being at TED .\nREF  : th√¥ng ƒëi·ªáp c·ªßa t√¥i mu·ªën g·ª≠i t·ªõi m·ªçi ng∆∞·ªùi trong 3 ph√∫t ·ªü ƒë√¢y c·ªßa t√¥i , ƒë√≥ l√† t√¥i tr√¢n tr·ªçng c∆° h·ªôi ƒë∆∞·ª£c chia s·∫ª c√πng c√°c b·∫°n . T√¥i s·∫Ω quay tr·ªü l·∫°i . T√¥i th√≠ch tham gia TED .\nPRED : tin nh·∫Øn c·ªßa t√¥i v·ªõi c√°c b·∫°n , t·ª´ ba ph√∫t c·ªßa t√¥i , ƒë√≥ l√† t√¥i r·∫•t tr√¢n tr·ªçng c∆° h·ªôi ƒë·ªÉ chia s·∫ª ƒëi·ªÅu n√†y v·ªõi c√°c b·∫°n . T√¥i y√™u c·∫ßu tr·ªü l·∫°i ·ªü TED .\n--------------------------------------------------\n‚úÖ Model saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13332/13332 [06:46<00:00, 32.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Train Loss: 2.7043, Val Loss: 2.7914\nüß™ Sample Translations (5 sentences):\n[1]\nEN   : And I would ask him every once in a while , \" Dad , do you want it to sound like the record ? \"\nREF  : ƒê√¥i khi , t√¥i h·ªèi √¥ng r·∫±ng : \" B·ªë ∆°i , b·ªë c√≥ mu·ªën ch∆°i gi·ªëng nh∆∞ b·∫£n g·ªëc kh√¥ng ·∫° ? \"\nPRED : t√¥i h·ªèi √¥ng ta m·ªói l·∫ßn trong m·ªôt th·ªùi gian , \" Dad , anh mu·ªën n√≥ nghe nh∆∞ ghi √¢m nh·∫°c ? \"\n--------------------------------------------------\n[2]\nEN   : So , we 're only going to look at the stars inside that small square , although we 've looked at all of them .\nREF  : V·∫≠y , ch√∫ng ta s·∫Ω ch·ªâ nh√¨n v√†o nh·ªØng ng√¥i sao b√™n trong h√¨nh vu√¥ng nh·ªè ƒë√≥ , m·∫∑t d√π ch√∫ng ta ƒë√£ nh√¨n t·∫•t c·∫£ ch√∫ng .\nPRED : ch√∫ng ta ch·ªâ nh√¨n v√†o c√°c ng√¥i sao nh·ªè b√™n trong h√¨nh vu√¥ng nh·ªè , m·∫∑c d√π ch√∫ng ta nh√¨n v√†o t·∫•t c·∫£ ch√∫ng .\n--------------------------------------------------\n[3]\nEN   : And they travel out into the cosmos at the speed of light .\nREF  : V√† ch√∫ng ƒëi v√†o v≈© tr·ª• v·ªõi v·∫≠n t·ªëc √°nh s√°ng .\nPRED : ch√∫ng ƒëi v√†o t·ªëc ƒë·ªô √°nh s√°ng c·ªßa √°nh s√°ng .\n--------------------------------------------------\n[4]\nEN   : Whatever it is , if it 's really important , you owe it to yourself to look at this toolbox and the engine that it 's going to work on , and no engine works well without being warmed up .\nREF  : S·ª± ki·ªán n√†o ƒëi n·ªØa , n·∫øu n√≥ r·∫•t quan tr·ªçng , b·∫°n t·ª± th·∫•y m√¨nh c·∫ßn ƒë·∫øn c√¥ng c·ª• n√†y v√† c·ªó m√°y s·∫Ω ƒë∆∞·ª£c v·∫∑n l√™n , v√† kh√¥ng m√°y n√†o ch·∫°y t·ªët n·∫øu kh√¥ng ƒë∆∞·ª£c l√†m ·∫•m .\nPRED : n·∫øu n√≥ r·∫•t quan tr·ªçng , n·∫øu n√≥ r·∫•t quan tr·ªçng , b·∫°n s·∫Ω t·ª± nh·ªß n√≥ v√†o h·ªôp d·ª•ng c·ª• n√†y v√† ƒë·ªông c∆° m√† n√≥ s·∫Ω ho·∫°t ƒë·ªông t·ªët h∆°n , v√† kh√¥ng c√≥ ƒë·ªông c∆° kh√≠ n√†o c√≥ th·ªÉ b·ªã chi·∫øn tranh .\n--------------------------------------------------\n[5]\nEN   : So , my message to you folks , from my three minutes , is that I appreciate the chance to share this with you . I will be back . I love being at TED .\nREF  : th√¥ng ƒëi·ªáp c·ªßa t√¥i mu·ªën g·ª≠i t·ªõi m·ªçi ng∆∞·ªùi trong 3 ph√∫t ·ªü ƒë√¢y c·ªßa t√¥i , ƒë√≥ l√† t√¥i tr√¢n tr·ªçng c∆° h·ªôi ƒë∆∞·ª£c chia s·∫ª c√πng c√°c b·∫°n . T√¥i s·∫Ω quay tr·ªü l·∫°i . T√¥i th√≠ch tham gia TED .\nPRED : tin nh·∫Øn c·ªßa t√¥i v·ªõi c√°c b·∫°n , t·ª´ ba ph√∫t c·ªßa t√¥i , l√† t√¥i ƒë√°nh gi√° kh·∫£ nƒÉng chia s·∫ª v·ªõi b·∫°n . T√¥i s·∫Ω tr·ªü l·∫°i v·ªõi b·∫°n . T√¥i r·∫•t th√≠ch th√∫ khi ·ªü TED .\n--------------------------------------------------\n‚úÖ Model saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13332/13332 [06:43<00:00, 33.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Train Loss: 2.6919, Val Loss: 2.7811\nüß™ Sample Translations (5 sentences):\n[1]\nEN   : And I would ask him every once in a while , \" Dad , do you want it to sound like the record ? \"\nREF  : ƒê√¥i khi , t√¥i h·ªèi √¥ng r·∫±ng : \" B·ªë ∆°i , b·ªë c√≥ mu·ªën ch∆°i gi·ªëng nh∆∞ b·∫£n g·ªëc kh√¥ng ·∫° ? \"\nPRED : t√¥i h·ªèi √¥ng ta m·ªói l·∫ßn trong m·ªôt th·ªùi gian , \" B·ªë , anh mu·ªën n√≥ nghe nh∆∞ ghi √¢m ? \"\n--------------------------------------------------\n[2]\nEN   : So , we 're only going to look at the stars inside that small square , although we 've looked at all of them .\nREF  : V·∫≠y , ch√∫ng ta s·∫Ω ch·ªâ nh√¨n v√†o nh·ªØng ng√¥i sao b√™n trong h√¨nh vu√¥ng nh·ªè ƒë√≥ , m·∫∑t d√π ch√∫ng ta ƒë√£ nh√¨n t·∫•t c·∫£ ch√∫ng .\nPRED : ch√∫ng ta ch·ªâ nh√¨n v√†o nh·ªØng ng√¥i sao nh·ªè b√© nh·ªè , m·∫∑c d√π ch√∫ng ta nh√¨n v√†o t·∫•t c·∫£ ch√∫ng .\n--------------------------------------------------\n[3]\nEN   : And they travel out into the cosmos at the speed of light .\nREF  : V√† ch√∫ng ƒëi v√†o v≈© tr·ª• v·ªõi v·∫≠n t·ªëc √°nh s√°ng .\nPRED : ch√∫ng ƒëi v√†o t·ªëc ƒë·ªô √°nh s√°ng c·ªßa √°nh s√°ng .\n--------------------------------------------------\n[4]\nEN   : Whatever it is , if it 's really important , you owe it to yourself to look at this toolbox and the engine that it 's going to work on , and no engine works well without being warmed up .\nREF  : S·ª± ki·ªán n√†o ƒëi n·ªØa , n·∫øu n√≥ r·∫•t quan tr·ªçng , b·∫°n t·ª± th·∫•y m√¨nh c·∫ßn ƒë·∫øn c√¥ng c·ª• n√†y v√† c·ªó m√°y s·∫Ω ƒë∆∞·ª£c v·∫∑n l√™n , v√† kh√¥ng m√°y n√†o ch·∫°y t·ªët n·∫øu kh√¥ng ƒë∆∞·ª£c l√†m ·∫•m .\nPRED : chƒÉng n·ªØa , n·∫øu n√≥ th·∫≠t s·ª± quan tr·ªçng , b·∫°n s·∫Ω t·ª± nh√©t n√≥ v√†o d·ª•ng c·ª• n√†y v√† ƒë·ªông c∆° m√† n√≥ s·∫Ω ho·∫°t ƒë·ªông tr√™n c√¥ng c·ª• n√†y v√† kh√¥ng c√≥ ƒë·ªông c∆° t·ªët .\n--------------------------------------------------\n[5]\nEN   : So , my message to you folks , from my three minutes , is that I appreciate the chance to share this with you . I will be back . I love being at TED .\nREF  : th√¥ng ƒëi·ªáp c·ªßa t√¥i mu·ªën g·ª≠i t·ªõi m·ªçi ng∆∞·ªùi trong 3 ph√∫t ·ªü ƒë√¢y c·ªßa t√¥i , ƒë√≥ l√† t√¥i tr√¢n tr·ªçng c∆° h·ªôi ƒë∆∞·ª£c chia s·∫ª c√πng c√°c b·∫°n . T√¥i s·∫Ω quay tr·ªü l·∫°i . T√¥i th√≠ch tham gia TED .\nPRED : g·ª≠i tin nh·∫Øn c·ªßa t√¥i v·ªõi c√°c b·∫°n , t·ª´ 3 ph√∫t , ƒë√≥ l√† t√¥i r·∫•t tr√¢n tr·ªçng c∆° h·ªôi ƒë·ªÉ chia s·∫ª v·ªõi c√°c b·∫°n . T√¥i s·∫Ω tr·ªü l·∫°i v·ªõi c√°c b·∫°n . T√¥i y√™u th√≠ch ·ªü TED .\n--------------------------------------------------\n‚úÖ Model saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13332/13332 [06:44<00:00, 32.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11, Train Loss: 2.6808, Val Loss: 2.7803\nüß™ Sample Translations (5 sentences):\n[1]\nEN   : And I would ask him every once in a while , \" Dad , do you want it to sound like the record ? \"\nREF  : ƒê√¥i khi , t√¥i h·ªèi √¥ng r·∫±ng : \" B·ªë ∆°i , b·ªë c√≥ mu·ªën ch∆°i gi·ªëng nh∆∞ b·∫£n g·ªëc kh√¥ng ·∫° ? \"\nPRED : t√¥i h·ªèi √¥ng ·∫•y m·ªói l·∫ßn trong m·ªôt th·ªùi gian , \" M·∫π , anh mu·ªën n√≥ nghe nh∆∞ ghi √¢m ? \"\n--------------------------------------------------\n[2]\nEN   : So , we 're only going to look at the stars inside that small square , although we 've looked at all of them .\nREF  : V·∫≠y , ch√∫ng ta s·∫Ω ch·ªâ nh√¨n v√†o nh·ªØng ng√¥i sao b√™n trong h√¨nh vu√¥ng nh·ªè ƒë√≥ , m·∫∑t d√π ch√∫ng ta ƒë√£ nh√¨n t·∫•t c·∫£ ch√∫ng .\nPRED : ch√∫ng ta ch·ªâ nh√¨n v√†o c√°c ng√¥i sao trong h√¨nh vu√¥ng nh·ªè , m·∫∑c d√π ch√∫ng ta nh√¨n v√†o t·∫•t c·∫£ ch√∫ng .\n--------------------------------------------------\n[3]\nEN   : And they travel out into the cosmos at the speed of light .\nREF  : V√† ch√∫ng ƒëi v√†o v≈© tr·ª• v·ªõi v·∫≠n t·ªëc √°nh s√°ng .\nPRED : ch√∫ng ƒëi v√†o t·ªëc ƒë·ªô c·ªßa √°nh s√°ng .\n--------------------------------------------------\n[4]\nEN   : Whatever it is , if it 's really important , you owe it to yourself to look at this toolbox and the engine that it 's going to work on , and no engine works well without being warmed up .\nREF  : S·ª± ki·ªán n√†o ƒëi n·ªØa , n·∫øu n√≥ r·∫•t quan tr·ªçng , b·∫°n t·ª± th·∫•y m√¨nh c·∫ßn ƒë·∫øn c√¥ng c·ª• n√†y v√† c·ªó m√°y s·∫Ω ƒë∆∞·ª£c v·∫∑n l√™n , v√† kh√¥ng m√°y n√†o ch·∫°y t·ªët n·∫øu kh√¥ng ƒë∆∞·ª£c l√†m ·∫•m .\nPRED : chƒÉng n·ªØa , n·∫øu n√≥ th·∫≠t s·ª± quan tr·ªçng , b·∫°n c√≥ th·ªÉ t·ª± nh√©t n√≥ v√†o d·ª•ng c·ª• n√†y v√† ƒë·ªông c∆° m√† n√≥ s·∫Ω ho·∫°t ƒë·ªông t·ªët kh√¥ng c√≥ chi·∫øn tranh .\n--------------------------------------------------\n[5]\nEN   : So , my message to you folks , from my three minutes , is that I appreciate the chance to share this with you . I will be back . I love being at TED .\nREF  : th√¥ng ƒëi·ªáp c·ªßa t√¥i mu·ªën g·ª≠i t·ªõi m·ªçi ng∆∞·ªùi trong 3 ph√∫t ·ªü ƒë√¢y c·ªßa t√¥i , ƒë√≥ l√† t√¥i tr√¢n tr·ªçng c∆° h·ªôi ƒë∆∞·ª£c chia s·∫ª c√πng c√°c b·∫°n . T√¥i s·∫Ω quay tr·ªü l·∫°i . T√¥i th√≠ch tham gia TED .\nPRED : tin nh·∫Øn c·ªßa t√¥i v·ªõi c√°c b·∫°n , t·ª´ ba ph√∫t c·ªßa t√¥i , l√† t√¥i r·∫•t tr√¢n tr·ªçng v·ªõi b·∫°n . T√¥i s·∫Ω tr·ªü l·∫°i v·ªõi b·∫°n . T√¥i y√™u t√¥i r·∫•t th√≠ch TED .\n--------------------------------------------------\n‚úÖ Model saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13332/13332 [06:45<00:00, 32.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12, Train Loss: 2.6693, Val Loss: 2.7690\nüß™ Sample Translations (5 sentences):\n[1]\nEN   : And I would ask him every once in a while , \" Dad , do you want it to sound like the record ? \"\nREF  : ƒê√¥i khi , t√¥i h·ªèi √¥ng r·∫±ng : \" B·ªë ∆°i , b·ªë c√≥ mu·ªën ch∆°i gi·ªëng nh∆∞ b·∫£n g·ªëc kh√¥ng ·∫° ? \"\nPRED : t√¥i h·ªèi √¥ng ta m·ªói khi trong m·ªôt th·ªùi gian , \" B·ªë ∆°i , anh mu·ªën √¢m thanh ƒë√≥ nh∆∞ ghi √¢m ? \"\n--------------------------------------------------\n[2]\nEN   : So , we 're only going to look at the stars inside that small square , although we 've looked at all of them .\nREF  : V·∫≠y , ch√∫ng ta s·∫Ω ch·ªâ nh√¨n v√†o nh·ªØng ng√¥i sao b√™n trong h√¨nh vu√¥ng nh·ªè ƒë√≥ , m·∫∑t d√π ch√∫ng ta ƒë√£ nh√¨n t·∫•t c·∫£ ch√∫ng .\nPRED : ch√∫ng ta ch·ªâ nh√¨n v√†o c√°c ng√¥i sao trong h√¨nh vu√¥ng nh·ªè , m·∫∑c d√π ch√∫ng ta nh√¨n v√†o t·∫•t c·∫£ ch√∫ng .\n--------------------------------------------------\n[3]\nEN   : And they travel out into the cosmos at the speed of light .\nREF  : V√† ch√∫ng ƒëi v√†o v≈© tr·ª• v·ªõi v·∫≠n t·ªëc √°nh s√°ng .\nPRED : ch√∫ng ƒëi v√†o t·ªëc ƒë·ªô √°nh s√°ng c·ªßa √°nh s√°ng .\n--------------------------------------------------\n[4]\nEN   : Whatever it is , if it 's really important , you owe it to yourself to look at this toolbox and the engine that it 's going to work on , and no engine works well without being warmed up .\nREF  : S·ª± ki·ªán n√†o ƒëi n·ªØa , n·∫øu n√≥ r·∫•t quan tr·ªçng , b·∫°n t·ª± th·∫•y m√¨nh c·∫ßn ƒë·∫øn c√¥ng c·ª• n√†y v√† c·ªó m√°y s·∫Ω ƒë∆∞·ª£c v·∫∑n l√™n , v√† kh√¥ng m√°y n√†o ch·∫°y t·ªët n·∫øu kh√¥ng ƒë∆∞·ª£c l√†m ·∫•m .\nPRED : chƒÉng n·ªØa , n·∫øu n√≥ th·ª±c s·ª± quan tr·ªçng , b·∫°n s·∫Ω t·ª± nh·ªß n√≥ v√†o c√¥ng c·ª• n√†y v√† ƒë·ªông c∆° m√† n√≥ s·∫Ω ho·∫°t ƒë·ªông t·ªët h∆°n , v√† kh√¥ng c√≥ ƒë·ªông c∆° ƒë∆∞·ª£c g√¨ c√≥ th·ªÉ b·ªã chi·∫øn tranh .\n--------------------------------------------------\n[5]\nEN   : So , my message to you folks , from my three minutes , is that I appreciate the chance to share this with you . I will be back . I love being at TED .\nREF  : th√¥ng ƒëi·ªáp c·ªßa t√¥i mu·ªën g·ª≠i t·ªõi m·ªçi ng∆∞·ªùi trong 3 ph√∫t ·ªü ƒë√¢y c·ªßa t√¥i , ƒë√≥ l√† t√¥i tr√¢n tr·ªçng c∆° h·ªôi ƒë∆∞·ª£c chia s·∫ª c√πng c√°c b·∫°n . T√¥i s·∫Ω quay tr·ªü l·∫°i . T√¥i th√≠ch tham gia TED .\nPRED : th√¥ng ƒëi·ªáp c·ªßa t√¥i v·ªõi c√°c b·∫°n , t·ª´ 3 ph√∫t , l√† t√¥i tr√¢n tr·ªçng c∆° h·ªôi ƒë·ªÉ chia s·∫ª ƒëi·ªÅu n√†y v·ªõi c√°c b·∫°n . T√¥i s·∫Ω y√™u c·∫ßu tr·ªü l·∫°i v·ªõi TED .\n--------------------------------------------------\n‚úÖ Model saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13332/13332 [06:46<00:00, 32.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13, Train Loss: 2.6603, Val Loss: 2.7634\nüß™ Sample Translations (5 sentences):\n[1]\nEN   : And I would ask him every once in a while , \" Dad , do you want it to sound like the record ? \"\nREF  : ƒê√¥i khi , t√¥i h·ªèi √¥ng r·∫±ng : \" B·ªë ∆°i , b·ªë c√≥ mu·ªën ch∆°i gi·ªëng nh∆∞ b·∫£n g·ªëc kh√¥ng ·∫° ? \"\nPRED : t√¥i h·ªèi √¥ng ta t·ª´ng c√≥ m·ªôt l·∫ßn trong m·ªôt l√∫c , \" Dad , anh mu·ªën √¢m thanh ƒë√≥ nghe nh∆∞ l√† b·∫£n ghi √¢m ? \"\n--------------------------------------------------\n[2]\nEN   : So , we 're only going to look at the stars inside that small square , although we 've looked at all of them .\nREF  : V·∫≠y , ch√∫ng ta s·∫Ω ch·ªâ nh√¨n v√†o nh·ªØng ng√¥i sao b√™n trong h√¨nh vu√¥ng nh·ªè ƒë√≥ , m·∫∑t d√π ch√∫ng ta ƒë√£ nh√¨n t·∫•t c·∫£ ch√∫ng .\nPRED : ch√∫ng ta ch·ªâ nh√¨n v√†o c√°c ng√¥i sao trong h√¨nh vu√¥ng nh·ªè , m·∫∑c d√π ch√∫ng ta nh√¨n v√†o t·∫•t c·∫£ ch√∫ng .\n--------------------------------------------------\n[3]\nEN   : And they travel out into the cosmos at the speed of light .\nREF  : V√† ch√∫ng ƒëi v√†o v≈© tr·ª• v·ªõi v·∫≠n t·ªëc √°nh s√°ng .\nPRED : ch√∫ng ƒëi v√†o t·ªëc ƒë·ªô √°nh s√°ng .\n--------------------------------------------------\n[4]\nEN   : Whatever it is , if it 's really important , you owe it to yourself to look at this toolbox and the engine that it 's going to work on , and no engine works well without being warmed up .\nREF  : S·ª± ki·ªán n√†o ƒëi n·ªØa , n·∫øu n√≥ r·∫•t quan tr·ªçng , b·∫°n t·ª± th·∫•y m√¨nh c·∫ßn ƒë·∫øn c√¥ng c·ª• n√†y v√† c·ªó m√°y s·∫Ω ƒë∆∞·ª£c v·∫∑n l√™n , v√† kh√¥ng m√°y n√†o ch·∫°y t·ªët n·∫øu kh√¥ng ƒë∆∞·ª£c l√†m ·∫•m .\nPRED : chƒÉng n·ªØa , n·∫øu n√≥ th·ª±c s·ª± quan tr·ªçng , b·∫°n n·ª£ n√≥ th√†nh c√¥ng c·ª• n√†y v√† ƒë·ªông c∆° m√† n√≥ s·∫Ω ho·∫°t ƒë·ªông tr√™n h·ªôp , v√† kh√¥ng c√≥ ƒë·ªông c∆° m√† kh√¥ng c√≥ ƒë·ªông c∆° ƒë∆∞·ª£c chi·∫øn tranh n√†o .\n--------------------------------------------------\n[5]\nEN   : So , my message to you folks , from my three minutes , is that I appreciate the chance to share this with you . I will be back . I love being at TED .\nREF  : th√¥ng ƒëi·ªáp c·ªßa t√¥i mu·ªën g·ª≠i t·ªõi m·ªçi ng∆∞·ªùi trong 3 ph√∫t ·ªü ƒë√¢y c·ªßa t√¥i , ƒë√≥ l√† t√¥i tr√¢n tr·ªçng c∆° h·ªôi ƒë∆∞·ª£c chia s·∫ª c√πng c√°c b·∫°n . T√¥i s·∫Ω quay tr·ªü l·∫°i . T√¥i th√≠ch tham gia TED .\nPRED : th√¥ng ƒëi·ªáp c·ªßa t√¥i v·ªõi c√°c b·∫°n , t·ª´ ba ph√∫t , l√† t√¥i tr√¢n tr·ªçng c∆° h·ªôi ƒë·ªÉ chia s·∫ª ƒëi·ªÅu n√†y v·ªõi c√°c b·∫°n . T√¥i s·∫Ω r·∫•t vui khi t√¥i ƒëang ·ªü TED . T√¥i y√™u th√≠ch ·ªü TED .\n--------------------------------------------------\n‚úÖ Model saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13332/13332 [06:47<00:00, 32.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14, Train Loss: 2.6498, Val Loss: 2.7593\nüß™ Sample Translations (5 sentences):\n[1]\nEN   : And I would ask him every once in a while , \" Dad , do you want it to sound like the record ? \"\nREF  : ƒê√¥i khi , t√¥i h·ªèi √¥ng r·∫±ng : \" B·ªë ∆°i , b·ªë c√≥ mu·ªën ch∆°i gi·ªëng nh∆∞ b·∫£n g·ªëc kh√¥ng ·∫° ? \"\nPRED : t√¥i h·ªèi anh ta m·ªói khi trong m·ªôt th·ªùi gian , \" Dad , anh mu·ªën n√≥ nghe √¢m thanh nh∆∞ b·∫£n ghi √¢m ? \"\n--------------------------------------------------\n[2]\nEN   : So , we 're only going to look at the stars inside that small square , although we 've looked at all of them .\nREF  : V·∫≠y , ch√∫ng ta s·∫Ω ch·ªâ nh√¨n v√†o nh·ªØng ng√¥i sao b√™n trong h√¨nh vu√¥ng nh·ªè ƒë√≥ , m·∫∑t d√π ch√∫ng ta ƒë√£ nh√¨n t·∫•t c·∫£ ch√∫ng .\nPRED : ch√∫ng ta ch·ªâ nh√¨n v√†o c√°c ng√¥i sao nh·ªè b√™n trong h√¨nh vu√¥ng nh·ªè , m·∫∑c d√π ch√∫ng ta nh√¨n v√†o t·∫•t c·∫£ ch√∫ng .\n--------------------------------------------------\n[3]\nEN   : And they travel out into the cosmos at the speed of light .\nREF  : V√† ch√∫ng ƒëi v√†o v≈© tr·ª• v·ªõi v·∫≠n t·ªëc √°nh s√°ng .\nPRED : ch√∫ng ƒëi v√†o trong v≈© tr·ª• t·ªëc ƒë·ªô √°nh s√°ng .\n--------------------------------------------------\n[4]\nEN   : Whatever it is , if it 's really important , you owe it to yourself to look at this toolbox and the engine that it 's going to work on , and no engine works well without being warmed up .\nREF  : S·ª± ki·ªán n√†o ƒëi n·ªØa , n·∫øu n√≥ r·∫•t quan tr·ªçng , b·∫°n t·ª± th·∫•y m√¨nh c·∫ßn ƒë·∫øn c√¥ng c·ª• n√†y v√† c·ªó m√°y s·∫Ω ƒë∆∞·ª£c v·∫∑n l√™n , v√† kh√¥ng m√°y n√†o ch·∫°y t·ªët n·∫øu kh√¥ng ƒë∆∞·ª£c l√†m ·∫•m .\nPRED : n√≥ l√† , n·∫øu n√≥ th·ª±c s·ª± quan tr·ªçng , b·∫°n s·∫Ω t·ª± nh·ªß n√≥ ƒë·ªÉ nh√¨n v√†o chi·∫øc h·ªôp n√†y v√† ƒë·ªông c∆° m√† n√≥ s·∫Ω ho·∫°t ƒë·ªông t·ªët , v√† kh√¥ng c√≥ ƒë·ªông c∆° n√†o m√† kh√¥ng c√≥ chi·∫øn tranh n√†o .\n--------------------------------------------------\n[5]\nEN   : So , my message to you folks , from my three minutes , is that I appreciate the chance to share this with you . I will be back . I love being at TED .\nREF  : th√¥ng ƒëi·ªáp c·ªßa t√¥i mu·ªën g·ª≠i t·ªõi m·ªçi ng∆∞·ªùi trong 3 ph√∫t ·ªü ƒë√¢y c·ªßa t√¥i , ƒë√≥ l√† t√¥i tr√¢n tr·ªçng c∆° h·ªôi ƒë∆∞·ª£c chia s·∫ª c√πng c√°c b·∫°n . T√¥i s·∫Ω quay tr·ªü l·∫°i . T√¥i th√≠ch tham gia TED .\nPRED : g·ª≠i tin nh·∫Øn cho c√°c b·∫°n , t·ª´ ba ph√∫t t√¥i , l√† t√¥i ƒë√°nh gi√° c∆° h·ªôi ƒë·ªÉ chia s·∫ª v·ªõi c√°c b·∫°n . T√¥i r·∫•t th√≠ch th√∫ v·ªõi TED . T√¥i y√™u t√¥i .\n--------------------------------------------------\n‚úÖ Model saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13332/13332 [06:51<00:00, 32.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15, Train Loss: 2.6419, Val Loss: 2.7579\nüß™ Sample Translations (5 sentences):\n[1]\nEN   : And I would ask him every once in a while , \" Dad , do you want it to sound like the record ? \"\nREF  : ƒê√¥i khi , t√¥i h·ªèi √¥ng r·∫±ng : \" B·ªë ∆°i , b·ªë c√≥ mu·ªën ch∆°i gi·ªëng nh∆∞ b·∫£n g·ªëc kh√¥ng ·∫° ? \"\nPRED : t√¥i s·∫Ω h·ªèi anh ta m·ªói l·∫ßn trong m·ªôt l√∫c , \" B·ªë , anh mu·ªën n√≥ nghe nh∆∞ b·∫£n l∆∞u tr·ªØ ? \"\n--------------------------------------------------\n[2]\nEN   : So , we 're only going to look at the stars inside that small square , although we 've looked at all of them .\nREF  : V·∫≠y , ch√∫ng ta s·∫Ω ch·ªâ nh√¨n v√†o nh·ªØng ng√¥i sao b√™n trong h√¨nh vu√¥ng nh·ªè ƒë√≥ , m·∫∑t d√π ch√∫ng ta ƒë√£ nh√¨n t·∫•t c·∫£ ch√∫ng .\nPRED : ch√∫ng ta ch·ªâ nh√¨n v√†o c√°c ng√¥i sao nh·ªè b√™n trong h√¨nh vu√¥ng nh·ªè , m·∫∑c d√π ch√∫ng ta nh√¨n v√†o t·∫•t c·∫£ ch√∫ng .\n--------------------------------------------------\n[3]\nEN   : And they travel out into the cosmos at the speed of light .\nREF  : V√† ch√∫ng ƒëi v√†o v≈© tr·ª• v·ªõi v·∫≠n t·ªëc √°nh s√°ng .\nPRED : ch√∫ng ƒëi v√†o t·ªëc ƒë·ªô c·ªßa v≈© tr·ª• .\n--------------------------------------------------\n[4]\nEN   : Whatever it is , if it 's really important , you owe it to yourself to look at this toolbox and the engine that it 's going to work on , and no engine works well without being warmed up .\nREF  : S·ª± ki·ªán n√†o ƒëi n·ªØa , n·∫øu n√≥ r·∫•t quan tr·ªçng , b·∫°n t·ª± th·∫•y m√¨nh c·∫ßn ƒë·∫øn c√¥ng c·ª• n√†y v√† c·ªó m√°y s·∫Ω ƒë∆∞·ª£c v·∫∑n l√™n , v√† kh√¥ng m√°y n√†o ch·∫°y t·ªët n·∫øu kh√¥ng ƒë∆∞·ª£c l√†m ·∫•m .\nPRED : n√≥ l√† , n·∫øu n√≥ th·ª±c s·ª± quan tr·ªçng , b·∫°n n·ª£ n√≥ v√†o h·ªôp d·ª•ng c·ª• n√†y v√† ƒë·ªông c∆° m√† n√≥ s·∫Ω ho·∫°t ƒë·ªông tr√™n m√°y , v√† kh√¥ng c√≥ hi·ªáu qu·∫£ g√¨ ƒë∆∞·ª£c t·∫°o ra , v√† kh√¥ng c√≥ t√°c d·ª•ng c·ª• th·ªÉ n√†o ƒë∆∞·ª£c .\n--------------------------------------------------\n[5]\nEN   : So , my message to you folks , from my three minutes , is that I appreciate the chance to share this with you . I will be back . I love being at TED .\nREF  : th√¥ng ƒëi·ªáp c·ªßa t√¥i mu·ªën g·ª≠i t·ªõi m·ªçi ng∆∞·ªùi trong 3 ph√∫t ·ªü ƒë√¢y c·ªßa t√¥i , ƒë√≥ l√† t√¥i tr√¢n tr·ªçng c∆° h·ªôi ƒë∆∞·ª£c chia s·∫ª c√πng c√°c b·∫°n . T√¥i s·∫Ω quay tr·ªü l·∫°i . T√¥i th√≠ch tham gia TED .\nPRED : g·ª≠i tin nh·∫Øn c·ªßa t√¥i ƒë·∫øn v·ªõi c√°c b·∫°n , t·ª´ ba ph√∫t , t√¥i nh·∫≠n ra c∆° h·ªôi ƒë·ªÉ chia s·∫ª v·ªõi c√°c b·∫°n . T√¥i s·∫Ω tr·ªü l·∫°i v·ªõi c√°c b·∫°n . T√¥i y√™u t√¥i y√™u t√¥i .\n--------------------------------------------------\n‚úÖ Model saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13332/13332 [06:52<00:00, 32.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16, Train Loss: 2.6333, Val Loss: 2.7550\nüß™ Sample Translations (5 sentences):\n[1]\nEN   : And I would ask him every once in a while , \" Dad , do you want it to sound like the record ? \"\nREF  : ƒê√¥i khi , t√¥i h·ªèi √¥ng r·∫±ng : \" B·ªë ∆°i , b·ªë c√≥ mu·ªën ch∆°i gi·ªëng nh∆∞ b·∫£n g·ªëc kh√¥ng ·∫° ? \"\nPRED : h·ªèi anh ·∫•y m·ªôt l·∫ßn trong m·ªôt l√∫c , \" B·ªë , anh mu·ªën n√≥ nghe nh∆∞ b·∫£n ghi √¢m ? \"\n--------------------------------------------------\n[2]\nEN   : So , we 're only going to look at the stars inside that small square , although we 've looked at all of them .\nREF  : V·∫≠y , ch√∫ng ta s·∫Ω ch·ªâ nh√¨n v√†o nh·ªØng ng√¥i sao b√™n trong h√¨nh vu√¥ng nh·ªè ƒë√≥ , m·∫∑t d√π ch√∫ng ta ƒë√£ nh√¨n t·∫•t c·∫£ ch√∫ng .\nPRED : ch√∫ng ta ch·ªâ nh√¨n v√†o c√°c ng√¥i sao nh·ªè b√™n trong h√¨nh vu√¥ng nh·ªè , m·∫∑c d√π ch√∫ng ta nh√¨n v√†o t·∫•t c·∫£ ch√∫ng .\n--------------------------------------------------\n[3]\nEN   : And they travel out into the cosmos at the speed of light .\nREF  : V√† ch√∫ng ƒëi v√†o v≈© tr·ª• v·ªõi v·∫≠n t·ªëc √°nh s√°ng .\nPRED : ch√∫ng ƒëi v√†o t·ªëc ƒë·ªô √°nh s√°ng .\n--------------------------------------------------\n[4]\nEN   : Whatever it is , if it 's really important , you owe it to yourself to look at this toolbox and the engine that it 's going to work on , and no engine works well without being warmed up .\nREF  : S·ª± ki·ªán n√†o ƒëi n·ªØa , n·∫øu n√≥ r·∫•t quan tr·ªçng , b·∫°n t·ª± th·∫•y m√¨nh c·∫ßn ƒë·∫øn c√¥ng c·ª• n√†y v√† c·ªó m√°y s·∫Ω ƒë∆∞·ª£c v·∫∑n l√™n , v√† kh√¥ng m√°y n√†o ch·∫°y t·ªët n·∫øu kh√¥ng ƒë∆∞·ª£c l√†m ·∫•m .\nPRED : n√≥ l√† , n·∫øu n√≥ th·∫≠t s·ª± quan tr·ªçng , b·∫°n n·ª£ n√≥ v√†o h·ªôp d·ª•ng c·ª• n√†y v√† ƒë·ªông c∆° m√† n√≥ s·∫Ω ho·∫°t ƒë·ªông tr√™n m√°y , v√† kh√¥ng c√≥ hi·ªáu qu·∫£ g√¨ ƒë∆∞·ª£c √°p d·ª•ng .\n--------------------------------------------------\n[5]\nEN   : So , my message to you folks , from my three minutes , is that I appreciate the chance to share this with you . I will be back . I love being at TED .\nREF  : th√¥ng ƒëi·ªáp c·ªßa t√¥i mu·ªën g·ª≠i t·ªõi m·ªçi ng∆∞·ªùi trong 3 ph√∫t ·ªü ƒë√¢y c·ªßa t√¥i , ƒë√≥ l√† t√¥i tr√¢n tr·ªçng c∆° h·ªôi ƒë∆∞·ª£c chia s·∫ª c√πng c√°c b·∫°n . T√¥i s·∫Ω quay tr·ªü l·∫°i . T√¥i th√≠ch tham gia TED .\nPRED : tin nh·∫Øn c·ªßa t√¥i v·ªõi c√°c b·∫°n , t·ª´ ba ph√∫t , ƒë√≥ l√† t√¥i ƒë√°nh gi√° c∆° h·ªôi ƒë·ªÉ chia s·∫ª v·ªõi c√°c b·∫°n . T√¥i s·∫Ω tr·ªü l·∫°i . T√¥i y√™u anh ·ªü TED .\n--------------------------------------------------\n‚úÖ Model saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13332/13332 [06:52<00:00, 32.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17, Train Loss: 2.6249, Val Loss: 2.7519\nüß™ Sample Translations (5 sentences):\n[1]\nEN   : And I would ask him every once in a while , \" Dad , do you want it to sound like the record ? \"\nREF  : ƒê√¥i khi , t√¥i h·ªèi √¥ng r·∫±ng : \" B·ªë ∆°i , b·ªë c√≥ mu·ªën ch∆°i gi·ªëng nh∆∞ b·∫£n g·ªëc kh√¥ng ·∫° ? \"\nPRED : t√¥i h·ªèi anh ta m·ªói khi trong m·ªôt th·ªùi gian , \" B·ªë ∆°i , anh mu·ªën n√≥ nh∆∞ b·∫£n ghi √¢m ? \"\n--------------------------------------------------\n[2]\nEN   : So , we 're only going to look at the stars inside that small square , although we 've looked at all of them .\nREF  : V·∫≠y , ch√∫ng ta s·∫Ω ch·ªâ nh√¨n v√†o nh·ªØng ng√¥i sao b√™n trong h√¨nh vu√¥ng nh·ªè ƒë√≥ , m·∫∑t d√π ch√∫ng ta ƒë√£ nh√¨n t·∫•t c·∫£ ch√∫ng .\nPRED : ch√∫ng ta ch·ªâ nh√¨n v√†o c√°c ng√¥i sao trong ng√¥i sao nh·ªè ƒë√≥ , m·∫∑c d√π ch√∫ng ta nh√¨n v√†o t·∫•t c·∫£ ch√∫ng .\n--------------------------------------------------\n[3]\nEN   : And they travel out into the cosmos at the speed of light .\nREF  : V√† ch√∫ng ƒëi v√†o v≈© tr·ª• v·ªõi v·∫≠n t·ªëc √°nh s√°ng .\nPRED : ch√∫ng ƒëi v√†o nh·ªØng v≈© tr·ª• t·ªëc ƒë·ªô √°nh s√°ng .\n--------------------------------------------------\n[4]\nEN   : Whatever it is , if it 's really important , you owe it to yourself to look at this toolbox and the engine that it 's going to work on , and no engine works well without being warmed up .\nREF  : S·ª± ki·ªán n√†o ƒëi n·ªØa , n·∫øu n√≥ r·∫•t quan tr·ªçng , b·∫°n t·ª± th·∫•y m√¨nh c·∫ßn ƒë·∫øn c√¥ng c·ª• n√†y v√† c·ªó m√°y s·∫Ω ƒë∆∞·ª£c v·∫∑n l√™n , v√† kh√¥ng m√°y n√†o ch·∫°y t·ªët n·∫øu kh√¥ng ƒë∆∞·ª£c l√†m ·∫•m .\nPRED : n√≥ l√† , n·∫øu n√≥ th·ª±c s·ª± quan tr·ªçng , b·∫°n n·ª£ n√≥ v√†o chi·∫øc h·ªôp n√†y v√† ƒë·ªông c∆° m√† n√≥ s·∫Ω ho·∫°t ƒë·ªông tr√™n chi·∫øc h·ªôp n√†y v√† kh√¥ng c√≥ hi·ªáu qu·∫£ g√¨ c·∫£ .\n--------------------------------------------------\n[5]\nEN   : So , my message to you folks , from my three minutes , is that I appreciate the chance to share this with you . I will be back . I love being at TED .\nREF  : th√¥ng ƒëi·ªáp c·ªßa t√¥i mu·ªën g·ª≠i t·ªõi m·ªçi ng∆∞·ªùi trong 3 ph√∫t ·ªü ƒë√¢y c·ªßa t√¥i , ƒë√≥ l√† t√¥i tr√¢n tr·ªçng c∆° h·ªôi ƒë∆∞·ª£c chia s·∫ª c√πng c√°c b·∫°n . T√¥i s·∫Ω quay tr·ªü l·∫°i . T√¥i th√≠ch tham gia TED .\nPRED : g·ª≠i tin nh·∫Øn cho c√°c b·∫°n , t·ª´ 3 ph√∫t , l√† t√¥i ƒë√°nh gi√° c∆° h·ªôi ƒë·ªÉ chia s·∫ª v·ªõi c√°c b·∫°n . T√¥i s·∫Ω y√™u c·∫ßu tr·ªü l·∫°i v·ªõi c√°c b·∫°n .\n--------------------------------------------------\n‚úÖ Model saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13332/13332 [06:51<00:00, 32.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18, Train Loss: 2.6165, Val Loss: 2.7483\nüß™ Sample Translations (5 sentences):\n[1]\nEN   : And I would ask him every once in a while , \" Dad , do you want it to sound like the record ? \"\nREF  : ƒê√¥i khi , t√¥i h·ªèi √¥ng r·∫±ng : \" B·ªë ∆°i , b·ªë c√≥ mu·ªën ch∆°i gi·ªëng nh∆∞ b·∫£n g·ªëc kh√¥ng ·∫° ? \"\nPRED : h·ªèi anh ta m·ªói khi , \" Dad , anh mu·ªën n√≥ nghe nh∆∞ b·∫£n ghi √¢m ? \"\n--------------------------------------------------\n[2]\nEN   : So , we 're only going to look at the stars inside that small square , although we 've looked at all of them .\nREF  : V·∫≠y , ch√∫ng ta s·∫Ω ch·ªâ nh√¨n v√†o nh·ªØng ng√¥i sao b√™n trong h√¨nh vu√¥ng nh·ªè ƒë√≥ , m·∫∑t d√π ch√∫ng ta ƒë√£ nh√¨n t·∫•t c·∫£ ch√∫ng .\nPRED : ch√∫ng ta ch·ªâ nh√¨n v√†o c√°c ng√¥i sao nh·ªè b√™n trong h√¨nh vu√¥ng nh·ªè ƒë√≥ , m·∫∑c d√π ch√∫ng ta nh√¨n v√†o ch√∫ng .\n--------------------------------------------------\n[3]\nEN   : And they travel out into the cosmos at the speed of light .\nREF  : V√† ch√∫ng ƒëi v√†o v≈© tr·ª• v·ªõi v·∫≠n t·ªëc √°nh s√°ng .\nPRED : ch√∫ng ƒëi v√†o trong v≈© tr·ª• t·ªëc ƒë·ªô √°nh s√°ng .\n--------------------------------------------------\n[4]\nEN   : Whatever it is , if it 's really important , you owe it to yourself to look at this toolbox and the engine that it 's going to work on , and no engine works well without being warmed up .\nREF  : S·ª± ki·ªán n√†o ƒëi n·ªØa , n·∫øu n√≥ r·∫•t quan tr·ªçng , b·∫°n t·ª± th·∫•y m√¨nh c·∫ßn ƒë·∫øn c√¥ng c·ª• n√†y v√† c·ªó m√°y s·∫Ω ƒë∆∞·ª£c v·∫∑n l√™n , v√† kh√¥ng m√°y n√†o ch·∫°y t·ªët n·∫øu kh√¥ng ƒë∆∞·ª£c l√†m ·∫•m .\nPRED : chƒÉng n·ªØa , n·∫øu n√≥ th·ª±c s·ª± quan tr·ªçng , b·∫°n s·∫Ω t·ª± coi n√≥ l√† g√¨ , b·∫°n s·∫Ω t·ª± l√†m vi·ªác tr√™n chi·∫øc h·ªôp n√†y v√† ƒë·ªông c∆° m√† n√≥ s·∫Ω kh√¥ng c√≥ hi·ªáu qu·∫£ n√†o b·ªã chi·∫øn tranh .\n--------------------------------------------------\n[5]\nEN   : So , my message to you folks , from my three minutes , is that I appreciate the chance to share this with you . I will be back . I love being at TED .\nREF  : th√¥ng ƒëi·ªáp c·ªßa t√¥i mu·ªën g·ª≠i t·ªõi m·ªçi ng∆∞·ªùi trong 3 ph√∫t ·ªü ƒë√¢y c·ªßa t√¥i , ƒë√≥ l√† t√¥i tr√¢n tr·ªçng c∆° h·ªôi ƒë∆∞·ª£c chia s·∫ª c√πng c√°c b·∫°n . T√¥i s·∫Ω quay tr·ªü l·∫°i . T√¥i th√≠ch tham gia TED .\nPRED : g·ª≠i tin nh·∫Øn c·ªßa t√¥i cho c√°c b·∫°n , t·ª´ ba ph√∫t , l√† t√¥i tr√¢n tr·ªçng c∆° h·ªôi ƒë·ªÉ chia s·∫ª v·ªõi c√°c b·∫°n . T√¥i s·∫Ω tr·ªü l·∫°i . T√¥i y√™u t√¥i r·∫•t th√≠ch ·ªü TED .\n--------------------------------------------------\n‚úÖ Model saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13332/13332 [06:51<00:00, 32.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19, Train Loss: 2.6108, Val Loss: 2.7461\nüß™ Sample Translations (5 sentences):\n[1]\nEN   : And I would ask him every once in a while , \" Dad , do you want it to sound like the record ? \"\nREF  : ƒê√¥i khi , t√¥i h·ªèi √¥ng r·∫±ng : \" B·ªë ∆°i , b·ªë c√≥ mu·ªën ch∆°i gi·ªëng nh∆∞ b·∫£n g·ªëc kh√¥ng ·∫° ? \"\nPRED : t√¥i s·∫Ω h·ªèi anh ta m·ªói khi , \" B·ªë ∆°i , anh mu·ªën n√≥ nghe nh∆∞ b·∫£n ghi √¢m ? \"\n--------------------------------------------------\n[2]\nEN   : So , we 're only going to look at the stars inside that small square , although we 've looked at all of them .\nREF  : V·∫≠y , ch√∫ng ta s·∫Ω ch·ªâ nh√¨n v√†o nh·ªØng ng√¥i sao b√™n trong h√¨nh vu√¥ng nh·ªè ƒë√≥ , m·∫∑t d√π ch√∫ng ta ƒë√£ nh√¨n t·∫•t c·∫£ ch√∫ng .\nPRED : ch√∫ng t√¥i ch·ªâ nh√¨n v√†o c√°c ng√¥i sao nh·ªè b√™n trong h√¨nh vu√¥ng nh·ªè , m·∫∑c d√π ch√∫ng t√¥i ƒë√£ nh√¨n v√†o ch√∫ng .\n--------------------------------------------------\n[3]\nEN   : And they travel out into the cosmos at the speed of light .\nREF  : V√† ch√∫ng ƒëi v√†o v≈© tr·ª• v·ªõi v·∫≠n t·ªëc √°nh s√°ng .\nPRED : ch√∫ng ƒëi ra ngo√†i v≈© tr·ª• t·ªëc ƒë·ªô .\n--------------------------------------------------\n[4]\nEN   : Whatever it is , if it 's really important , you owe it to yourself to look at this toolbox and the engine that it 's going to work on , and no engine works well without being warmed up .\nREF  : S·ª± ki·ªán n√†o ƒëi n·ªØa , n·∫øu n√≥ r·∫•t quan tr·ªçng , b·∫°n t·ª± th·∫•y m√¨nh c·∫ßn ƒë·∫øn c√¥ng c·ª• n√†y v√† c·ªó m√°y s·∫Ω ƒë∆∞·ª£c v·∫∑n l√™n , v√† kh√¥ng m√°y n√†o ch·∫°y t·ªët n·∫øu kh√¥ng ƒë∆∞·ª£c l√†m ·∫•m .\nPRED : n√≥ l√† , n·∫øu n√≥ th·ª±c s·ª± quan tr·ªçng , b·∫°n n·ª£ n√≥ v√†o h·ªôp h·ªôp h·ªôp n√†y v√† ƒë·ªông c∆° m√† n√≥ s·∫Ω ho·∫°t ƒë·ªông tr√™n chi·∫øc h·ªôp n√†y v√† ƒë·ªông c∆° m√† n√≥ s·∫Ω kh√¥ng c√≥ chi·∫øn tranh n√†o .\n--------------------------------------------------\n[5]\nEN   : So , my message to you folks , from my three minutes , is that I appreciate the chance to share this with you . I will be back . I love being at TED .\nREF  : th√¥ng ƒëi·ªáp c·ªßa t√¥i mu·ªën g·ª≠i t·ªõi m·ªçi ng∆∞·ªùi trong 3 ph√∫t ·ªü ƒë√¢y c·ªßa t√¥i , ƒë√≥ l√† t√¥i tr√¢n tr·ªçng c∆° h·ªôi ƒë∆∞·ª£c chia s·∫ª c√πng c√°c b·∫°n . T√¥i s·∫Ω quay tr·ªü l·∫°i . T√¥i th√≠ch tham gia TED .\nPRED : g·ª≠i tin nh·∫Øn c·ªßa t√¥i cho c√°c b·∫°n , t·ª´ 3 ph√∫t , t√¥i r·∫•t tr√¢n tr·ªçng c∆° h·ªôi ƒë·ªÉ chia s·∫ª v·ªõi c√°c b·∫°n . T√¥i s·∫Ω tr·ªü l·∫°i . T√¥i y√™u ƒë∆∞·ª£c ·ªü TED .\n--------------------------------------------------\n‚úÖ Model saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13332/13332 [06:51<00:00, 32.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20, Train Loss: 2.6035, Val Loss: 2.7440\nüß™ Sample Translations (5 sentences):\n[1]\nEN   : And I would ask him every once in a while , \" Dad , do you want it to sound like the record ? \"\nREF  : ƒê√¥i khi , t√¥i h·ªèi √¥ng r·∫±ng : \" B·ªë ∆°i , b·ªë c√≥ mu·ªën ch∆°i gi·ªëng nh∆∞ b·∫£n g·ªëc kh√¥ng ·∫° ? \"\nPRED : h·ªèi √¥ng ·∫•y m·ªôt l·∫ßn trong m·ªôt l√∫c , \" Dad , anh mu·ªën n√≥ nghe nh∆∞ b·∫£n ghi √¢m ? \"\n--------------------------------------------------\n[2]\nEN   : So , we 're only going to look at the stars inside that small square , although we 've looked at all of them .\nREF  : V·∫≠y , ch√∫ng ta s·∫Ω ch·ªâ nh√¨n v√†o nh·ªØng ng√¥i sao b√™n trong h√¨nh vu√¥ng nh·ªè ƒë√≥ , m·∫∑t d√π ch√∫ng ta ƒë√£ nh√¨n t·∫•t c·∫£ ch√∫ng .\nPRED : ch√∫ng ta ch·ªâ nh√¨n v√†o nh·ªØng ng√¥i sao nh·ªè b√™n trong h√¨nh vu√¥ng nh·ªè , m·∫∑c d√π ch√∫ng ta ƒë√£ nh√¨n v√†o t·∫•t c·∫£ ch√∫ng .\n--------------------------------------------------\n[3]\nEN   : And they travel out into the cosmos at the speed of light .\nREF  : V√† ch√∫ng ƒëi v√†o v≈© tr·ª• v·ªõi v·∫≠n t·ªëc √°nh s√°ng .\nPRED : ch√∫ng ƒëi v√†o t·ªëc ƒë·ªô c·ªßa √°nh s√°ng .\n--------------------------------------------------\n[4]\nEN   : Whatever it is , if it 's really important , you owe it to yourself to look at this toolbox and the engine that it 's going to work on , and no engine works well without being warmed up .\nREF  : S·ª± ki·ªán n√†o ƒëi n·ªØa , n·∫øu n√≥ r·∫•t quan tr·ªçng , b·∫°n t·ª± th·∫•y m√¨nh c·∫ßn ƒë·∫øn c√¥ng c·ª• n√†y v√† c·ªó m√°y s·∫Ω ƒë∆∞·ª£c v·∫∑n l√™n , v√† kh√¥ng m√°y n√†o ch·∫°y t·ªët n·∫øu kh√¥ng ƒë∆∞·ª£c l√†m ·∫•m .\nPRED : chƒÉng n·ªØa , n·∫øu n√≥ th·ª±c s·ª± quan tr·ªçng , b·∫°n s·∫Ω t·ª± nh·ªß n√≥ ƒë·ªÉ nh√¨n v√†o chi·∫øc h·ªôp n√†y v√† ƒë·ªông c∆° m√† n√≥ s·∫Ω ho·∫°t ƒë·ªông t·ªët kh√¥ng c√≥ chi·∫øn tranh n√†o .\n--------------------------------------------------\n[5]\nEN   : So , my message to you folks , from my three minutes , is that I appreciate the chance to share this with you . I will be back . I love being at TED .\nREF  : th√¥ng ƒëi·ªáp c·ªßa t√¥i mu·ªën g·ª≠i t·ªõi m·ªçi ng∆∞·ªùi trong 3 ph√∫t ·ªü ƒë√¢y c·ªßa t√¥i , ƒë√≥ l√† t√¥i tr√¢n tr·ªçng c∆° h·ªôi ƒë∆∞·ª£c chia s·∫ª c√πng c√°c b·∫°n . T√¥i s·∫Ω quay tr·ªü l·∫°i . T√¥i th√≠ch tham gia TED .\nPRED : tin nh·∫Øn c·ªßa t√¥i v·ªõi c√°c b·∫°n , t·ª´ ba ph√∫t , l√† t√¥i ƒë√°nh gi√° c∆° h·ªôi ƒë·ªÉ chia s·∫ª ƒëi·ªÅu n√†y v·ªõi c√°c b·∫°n . T√¥i s·∫Ω y√™u t√¥i tr·ªü l·∫°i .\n--------------------------------------------------\n‚úÖ Model saved!\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"# Step 7: Evaluate Model\nimport evaluate\nfrom tqdm import tqdm\n\ndef evaluate_model_metrics(model, sp, test_data, max_len, output_dir=\"eval\"):\n    os.makedirs(output_dir, exist_ok=True)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    model.eval()\n    rouge = evaluate.load('rouge')\n    bleu = evaluate.load('sacrebleu')\n    predictions = []\n    references = []\n\n    def translate_sample(input_text):\n        input_sentence = f\"[EN] {input_text.strip()} [VI]\"\n        input_ids = [1] + sp.encode(input_sentence, out_type=int)\n        input_ids = input_ids[:max_len-1] + [2]\n        input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)\n\n        with torch.no_grad():\n            for _ in range(max_len - len(input_ids)):\n                out = model(input_tensor)\n                next_token = torch.argmax(out[:, -1, :], dim=-1).item()\n                if next_token == 2:\n                    break\n                input_tensor = torch.cat([input_tensor, torch.tensor([[next_token]], device=device)], dim=1)\n\n        decoded = sp.decode(input_tensor[0].tolist())\n        return decoded.split(\"[VI]\")[-1].strip() if \"[VI]\" in decoded else decoded.strip()\n\n    for item in tqdm(test_data[:200], desc=\"Evaluating\"):\n        input_text = item.split('[VI]')[0].replace('[EN]', '').strip()\n        ref = item.split('[VI]')[-1].strip()\n        pred = translate_sample(input_text)\n        predictions.append(pred)\n        references.append([ref])\n\n    rouge_result = rouge.compute(predictions=predictions, references=[r[0] for r in references])\n    bleu_result = bleu.compute(predictions=predictions, references=references)\n\n    print(\"ROUGE:\", rouge_result)\n    print(f\"BLEU: {bleu_result['score']:.2f}\")\n\n    with open(os.path.join(output_dir, \"eval_results.txt\"), 'w') as f:\n        f.write(f\"ROUGE: {rouge_result}\\n\")\n        f.write(f\"BLEU: {bleu_result['score']:.2f}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T14:06:18.335395Z","iopub.execute_input":"2025-05-18T14:06:18.335782Z","iopub.status.idle":"2025-05-18T14:06:18.348332Z","shell.execute_reply.started":"2025-05-18T14:06:18.335753Z","shell.execute_reply":"2025-05-18T14:06:18.347544Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"# Run Step 7\nevaluate_model_metrics(model, sp, test_data, max_len, output_dir=\"eval\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T14:06:21.899579Z","iopub.execute_input":"2025-05-18T14:06:21.900327Z","iopub.status.idle":"2025-05-18T14:06:49.038250Z","shell.execute_reply.started":"2025-05-18T14:06:21.900301Z","shell.execute_reply":"2025-05-18T14:06:49.037569Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc1889b640d344dba06f57087d776f9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97ba8a313a744c61a153d52f2023f582"}},"metadata":{}},{"name":"stderr","text":"Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:24<00:00,  8.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"ROUGE: {'rouge1': 0.615170469779794, 'rouge2': 0.3711888026499641, 'rougeL': 0.5191040570084184, 'rougeLsum': 0.5190000486754944}\nBLEU: 18.41\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"# Step 8: Translate Text\ndef translate(model, sp, input_text=None, test_data=None, max_len=128, num_examples=5):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    model.eval()\n    translations = []\n\n    def translate_sample(text):\n        input_sentence = f\"[EN] {text.strip()} [VI]\"\n        input_ids = [1] + sp.encode(input_sentence, out_type=int)\n        input_ids = input_ids[:max_len-1] + [2]\n        input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)\n\n        with torch.no_grad():\n            for _ in range(max_len - len(input_ids)):\n                out = model(input_tensor)\n                next_token = torch.argmax(out[:, -1, :], dim=-1).item()\n                if next_token == 2:\n                    break\n                input_tensor = torch.cat([input_tensor, torch.tensor([[next_token]], device=device)], dim=1)\n\n        decoded = sp.decode(input_tensor[0].tolist())\n        return decoded.split(\"[VI]\")[-1].strip() if \"[VI]\" in decoded else decoded.strip()\n\n    if input_text:\n        translation = translate_sample(input_text)\n        translations.append({\"input\": input_text, \"translation\": translation, \"reference\": None})\n\n    if test_data:\n        for item in test_data[:num_examples]:\n            en_part = item.split('[VI]')[0].replace('[EN]', '').strip()\n            reference = item.split('[VI]')[-1].strip()\n            translation = translate_sample(en_part)\n            translations.append({\"input\": en_part, \"translation\": translation, \"reference\": reference})\n\n    for i, t in enumerate(translations):\n        print(f\"\\nExample {i + 1}:\")\n        print(f\"Input (EN): {t['input']}\")\n        print(f\"Translation (VI): {t['translation']}\")\n        if t['reference']:\n            print(f\"Reference (VI): {t['reference']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T14:07:15.416802Z","iopub.execute_input":"2025-05-18T14:07:15.417101Z","iopub.status.idle":"2025-05-18T14:07:15.426600Z","shell.execute_reply.started":"2025-05-18T14:07:15.417082Z","shell.execute_reply":"2025-05-18T14:07:15.425857Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"# Run Step 8\ntranslate(model, sp, input_text=\"Hello, how are you?\", test_data=val_data, max_len=max_len, num_examples=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T14:21:03.315095Z","iopub.execute_input":"2025-05-18T14:21:03.315664Z","iopub.status.idle":"2025-05-18T14:21:04.218243Z","shell.execute_reply.started":"2025-05-18T14:21:03.315638Z","shell.execute_reply":"2025-05-18T14:21:04.217540Z"}},"outputs":[{"name":"stdout","text":"\nExample 1:\nInput (EN): Hello, how are you?\nTranslation (VI): trai : B·∫°n c√≥ th·ªÉ l√†m theo d√µi theo d√µi .\n\nExample 2:\nInput (EN): It 's already happening . It 's not science fiction .\nTranslation (VI): chƒÉng n·ªØa . N√≥ kh√¥ng ph·∫£i l√† khoa h·ªçc vi·ªÖn t∆∞·ªüng .\nReference (VI): ƒêi·ªÅu n√†y th·ª±c s·ª± ƒëang x·∫£y ra , kh√¥ng ph·∫£i trong khoa h·ªçc vi·ªÖn t∆∞·ªüng .\n\nExample 3:\nInput (EN): Anticipation made them happy .\nTranslation (VI): b·∫°i li·ªát khi·∫øn h·ªç h·∫°nh ph√∫c .\nReference (VI): Mong ƒë·ª£i khi·∫øn h·ªç vui .\n\nExample 4:\nInput (EN): He was 94 when this photograph was taken .\nTranslation (VI): trai 94 khi t·∫•m h√¨nh n√†y ƒë∆∞·ª£c ch·ª•p .\nReference (VI): B·ª©c ·∫£nh n√†y ch·ª•p khi √¥ng ·∫•y 94 tu·ªïi\n\nExample 5:\nInput (EN): Is this proposition true ? Is this theory a good theory ?\nTranslation (VI): g·ª£i √Ω ƒë√∫ng kh√¥ng ? L√Ω thuy·∫øt n√†y c√≥ ph·∫£i l√† l√Ω thuy·∫øt t·ªët kh√¥ng ?\nReference (VI): Li·ªáu ƒë·ªÅ xu·∫•t n√†y c√≥ ƒë√∫ng ? Li·ªáu l√Ω thuy·∫øt n√†y c√≥ l√† m·ªôt l√Ω thuy·∫øt t·ªët ?\n\nExample 6:\nInput (EN): Why ? Because there happy to be doing something that 's the right thing to do .\nTranslation (VI): ? V√¨ c√≥ g√¨ ƒë√≥ h·∫°nh ph√∫c ƒë·ªÉ l√†m ƒëi·ªÅu g√¨ ƒë√≥ ƒë√∫ng ƒë·∫Øn .\nReference (VI): T·∫°i sao ? B·ªüi v√¨ h·ªç vui s∆∞·ªõng ƒë∆∞·ª£c l√†m ƒëi·ªÅu g√¨ ƒë√≥ ƒë√∫ng ƒë·∫Øn .\n\nExample 7:\nInput (EN): \" Yeah , well of course , I 'm looking after my patient . \"\nTranslation (VI): , t·∫•t nhi√™n , t√¥i ƒëang t√¨m th·∫•y b·ªánh nh√¢n c·ªßa m√¨nh . \"\nReference (VI): \" ·∫¨y , T·∫•t nhi√™n l√† c√≥ , t√¥i ƒëang theo d√µi b·ªánh nh√¢n c·ªßa t√¥i ch·ª© . \"\n\nExample 8:\nInput (EN): Can you see how it 's growing ? And how hundreds of millions and billions is coming out of poverty in Asia ?\nTranslation (VI): b·∫°n c√≥ th·ªÉ th·∫•y n√≥ ƒëang ph√°t tri·ªÉn nh∆∞ th·∫ø n√†o ? V√† h√†ng trƒÉm tri·ªáu v√† h√†ng t·ªâ ng∆∞·ªùi ƒëang ƒëi ra kh·ªèi ngh√®o ƒë√≥i ·ªü ch√¢u √Å ?\nReference (VI): C√°c b·∫°n c√≥ th·∫•y ƒë∆∞·ª£c ch√∫ng ph√°t tri·ªÉn nh∆∞ th·∫ø n√†o kh√¥ng ? V√† l√†m th·∫ø n√†o ƒë·ªÉ h√†ng t·ªâ ng∆∞·ªùi ƒëang tho√°t ra kh·ªèi ngh√®o ƒë√≥i ·ªü Ch√¢u √Å ?\n\nExample 9:\nInput (EN): That is not true , and if it were , life would be incredibly boring .\nTranslation (VI): chƒÉng n·ªØa , v√† n·∫øu n√≥ l√† s·ª± s·ªëng s·∫Ω v√¥ c√πng ch√°n n·∫£n .\nReference (VI): S·ª± th·∫≠t kh√¥ng ph·∫£i nh∆∞ v·∫≠y , v√† n·∫øu n√≥ l√† nh∆∞ v·∫≠y , th√¨ cu·ªôc ƒë·ªùi s·∫Ω tr·ªü n√™n v√¥ c√πng nh√†m ch√°n .\n\nExample 10:\nInput (EN): So what if you 've targeted cells ?\nTranslation (VI): n·∫øu b·∫°n nh·∫Øm ƒë·∫øn c√°c t·∫ø b√†o ?\nReference (VI): Trong th·∫ø gi·ªõi vaccine , n·∫øu ta k√≠ch th√≠ch c√°c t·∫ø b√†o ,\n\nExample 11:\nInput (EN): They 've already gained experience from raising their own children .\nTranslation (VI): g√°i n√†y ƒë√£ tr·∫£i nghi·ªám t·ª´ vi·ªác nu√¥i d·∫°y con c√°i m√¨nh .\nReference (VI): H·ªç ƒë√£ c√≥ s·∫µn kinh nghi·ªám khi nu√¥i con c·ªßa m√¨nh .\n","output_type":"stream"}],"execution_count":69}]}